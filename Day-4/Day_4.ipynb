{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF87DXKuDXeijlkzRk4yn7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmantec/AI-Agents-Crash-Course/blob/feat%2FDay-4/Day-4/Day_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk1yxj_FqUSz"
      },
      "outputs": [],
      "source": [
        "## Agents and Tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# So far, we have done:\n",
        "# - Day 1: Downloaded the data from a GitHub repository\n",
        "# - Day 2: Processed it by chunking it where necessary\n",
        "# - Day 3: Indexed the data so it's searchable\n",
        "\n",
        "# Note that it took us quite a lot of time. We're halfway through the course, and only now we started working on agents. Most of the time so far, we have spent on data preparation.\n",
        "\n",
        "# This is not a coincidence. Data preparation is the most time-consuming and critical part of building AI agents. Without properly prepared, cleaned, and indexed data, even the most sophisticated agent will provide poor results.\n",
        "\n",
        "# Now it's time to create an AI agent that will use this data through the search engine that we created yesterday.\n",
        "\n",
        "# This allows us to build context-aware agents. They can provide accurate, relevant answers based on your specific domain knowledge rather than just general training data.\n",
        "\n",
        "# In particular, we will:\n",
        "# - Learn what makes an AI system \"agentic\" through tool use\n",
        "# - Build an agent that can use the search function\n",
        "# - Use Pydantic AI to make it easier to implement agents\n",
        "\n",
        "# At the end of this lesson, you'll have a working AI Agent that you can answer your questions in a Jupyter notebook."
      ],
      "metadata": {
        "id": "jnquzfzBqXmp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. Tools and Agents\n",
        "\n",
        "# Agent - an agent is an LLM that can not only generate texts, but also invoke tools. Tools are external functions that the LLM can call in order to retrieve information, perform calculations, or take actions.\n",
        "\n",
        "# In our case, the agent needs to answer our questions using the content of the GitHub repository. So, the tool (only one) is a search(query).\n",
        "\n",
        "# But first, let's consider a situation where we have no tools at all. This is not an agent, it's just an LLM that can generate texts. Access to tools is what makes agents \"agentic\".\n"
      ],
      "metadata": {
        "id": "Z5hqIPUsqXir"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFgg3CSFuhaY",
        "outputId": "b55799c8-3475-4750-a8aa-54a61d2bb385"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uv\n",
            "  Downloading uv-0.8.22-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.8.22-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.8.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install openai minsearch requests python-frontmatter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VipVuB1unwt",
        "outputId": "9fb6d515-3f00-4373-ad46-fa58547043e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m32 packages\u001b[0m \u001b[2min 670ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 33ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mminsearch\u001b[0m\u001b[2m==0.0.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-frontmatter\u001b[0m\u001b[2m==1.1.0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "Upp2HntWvDNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the difference with an example.\n",
        "\n",
        "# We will try asking a question without giving the LLM access to search:\n",
        "\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "openai_client = OpenAI(api_key=userdata.get('GROQ_API_KEY'), base_url=\"https://api.groq.com/openai/v1\")\n",
        "\n",
        "user_prompt = \"I just discovered the course, can I joni now?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]\n",
        "\n",
        "response = openai_client.responses.create(\n",
        "    model='openai/gpt-oss-20b',\n",
        "    input=messages\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "AJlJHuvYqXhF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79747a5f-fa1f-4a75-c54b-1f43bff901c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure thing! I’d be happy to help you get on board.  \n",
            "Here’s what usually needs to happen to join a course:\n",
            "\n",
            "| Step | What to do | Why it matters |\n",
            "|------|------------|----------------|\n",
            "| **1️⃣ Create an account (if you don’t already have one)** | Sign‑up on the platform with your name, email, and a secure password. | You need a profile to track progress, receive updates, and interact with instructors or peers. |\n",
            "| **2️⃣ Find the course** | Navigate to the “Courses” or “Catalog” page, and search for the course title or code. | Makes sure you’re looking at the right offering (there can be multiple sections). |\n",
            "| **3️⃣ Check prerequisites & eligibility** | Read the syllabus or course description. Some courses require prior coursework, a certain GPA, or a specific level of proficiency. | Helps you avoid enrollment if you’re not ready, and saves you time. |\n",
            "| **4️⃣ Enroll / Register** | Click the “Enroll,” “Register,” or “Add to Cart” button. If it’s a paid course, you’ll be prompted to complete payment. | This officially puts you in the class roster and grants you access to materials. |\n",
            "| **5️⃣ Confirm enrollment** | You should receive a confirmation email and see the course listed in your dashboard. | Keeps you informed and lets you start planning your schedule. |\n",
            "\n",
            "### If you’re new to the platform\n",
            "- **Need an account?** Go to the home page and click **Sign Up** (or **Register**).  \n",
            "- **Forgot your password?** Use the “Forgot Password” link on the login screen.  \n",
            "- **Help with payment**? Most platforms accept credit cards, PayPal, or offer scholarships and financial aid.\n",
            "\n",
            "### Quick tips\n",
            "- **Enrollment deadline**: Some courses close enrollment a week before the start date. Check the calendar or course page for the cutoff.  \n",
            "- **Waiting list**: If the class is full, you can often join a waiting list. You’ll be notified if a spot opens.  \n",
            "- **Prerequisites**: If you’re missing a prerequisite, you can usually request an exception, but it’s best to reach out to the instructor or the admissions office.\n",
            "\n",
            "Let me know which specific course you’re interested in, or if you hit any snags along the way. I can guide you through any particular platform’s steps or help troubleshoot. Happy learning!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The response is generic. In our case, it's this:\n",
        "\n",
        "# “It depends on the course you're interested in. Many courses allow late enrollment, while others might have specific deadlines. I recommend checking the course's official website or contacting the instructor or administration for more details on joining.”\n",
        "\n",
        "# This answer is not really useful.\n",
        "\n",
        "# But if we let it invoke the search(query), the agent can give us a more useful answer.\n",
        "\n",
        "# Here's how the conversation would flow with our agent using the search tool:\n",
        "\n",
        "# - User: \"I just discovered the course, can I join now?\"\n",
        "# - Agent thinking: I can't answer this question, so I need to search for information about course enrollment and timing.\n",
        "# - Tool call: search(\"course enrollment join registration deadline\")\n",
        "# - Tool response: (...search results...)\n",
        "# - Agent response: \"Yes, you can still join the course even after the start date...\"\n",
        "\n",
        "# We will now explore how to implement it with OpenAI.\n"
      ],
      "metadata": {
        "id": "wMc_NxvJvMJ-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  find read_repo_data in the first lesson and sliding_window in the second lesson\n",
        "\n",
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import frontmatter\n",
        "\n",
        "def read_repo_data(repo_owner, repo_name):\n",
        "  \"\"\"\n",
        "  Download and parse all markdown files from a github repository\n",
        "\n",
        "  Args:\n",
        "    repo_owner : Github username or organization\n",
        "    repo_name: Repository name\n",
        "\n",
        "  Returns:\n",
        "    List of dictionaries containing file content and metadata\n",
        "  \"\"\"\n",
        "  prefix = 'https://codeload.github.com'\n",
        "  url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
        "  resp = requests.get(url)\n",
        "\n",
        "  if resp.status_code != 200:\n",
        "    raise Exception(f\"Failed to download repository {repo_owner}/{repo_name}: {resp.status_code}\")\n",
        "\n",
        "  repository_data = []\n",
        "\n",
        "  # Create a ZipFile object from the downloaded content\n",
        "  zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
        "\n",
        "  for file_info in zf.infolist():\n",
        "    filename = file_info.filename\n",
        "    filename_lower = filename.lower()\n",
        "\n",
        "    if not (filename_lower.endswith('.md') or (filename_lower.endswith('.mdx'))):\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      with zf.open(file_info) as f_in:\n",
        "        content = f_in.read().decode('utf-8', errors='ignore')\n",
        "        post = frontmatter.loads(content)\n",
        "        data = post.to_dict()\n",
        "        data['filename'] = filename\n",
        "        repository_data.append(data)\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing {filename}: {e}\")\n",
        "      continue\n",
        "\n",
        "  zf.close()\n",
        "  return repository_data"
      ],
      "metadata": {
        "id": "OGpBjgOm0oGO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now index this data with minsearch:\n",
        "\n",
        "from minsearch import Index\n",
        "\n",
        "# For DataTalksClub FAQ, it's similar, except we don't need to chunk the data. For the data engineering course, it'll look like this:\n",
        "\n",
        "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
        "\n",
        "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
        "\n",
        "faq_index = Index(\n",
        "    text_fields=[\"question\", \"content\"],\n",
        "    keyword_fields=[]\n",
        ")\n",
        "\n",
        "faq_index.fit(de_dtc_faq)\n",
        "\n",
        "query = 'Course: Can I still join the course after the start date?'\n",
        "results = faq_index.search(query)\n",
        "print(results)\n",
        "\n",
        "# This is text search, also known as \"lexical search\". We look for exact matches between our query and the documents."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB3y9pQl0n4j",
        "outputId": "2fedf9ea-3593-4e17-a614-ee49f85a05f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '33fc260cd8', 'question': 'Course: What can I do before the course starts?', 'sort_order': 5, 'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'}, {'id': 'bfafa427b3', 'question': 'Course: What are the prerequisites for this course?', 'sort_order': 2, 'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'}, {'id': '900f60fd25', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'sort_order': 15, 'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}, {'id': '4dbd2eea47', 'question': 'Homework: Are late submissions of homework allowed?', 'sort_order': 17, 'content': 'No, late submissions are not allowed. However, if the form is still open after the due date, you can still submit the homework. Confirm your submission by checking the date-timestamp on the Course page. Ensure you are logged in.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/017_4dbd2eea47_homework-are-late-submissions-of-homework-allowed.md'}, {'id': '52217fc51b', 'question': 'Course: I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'sort_order': 4, 'content': \"You don't need a confirmation email. You're accepted. You can start learning and submitting homework without registering. Registration was just to gauge interest before the start date.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/004_52217fc51b_course-i-have-registered-for-the-data-engineering.md'}, {'id': '721f9e0c29', 'question': 'How can we contribute to the course?', 'sort_order': 35, 'content': '- [Star the repository](https://github.com/DataTalksClub/data-engineering-zoomcamp).\\n- Share it with friends if you find it useful.\\n- Create a pull request (PR) if you can improve the text or structure of the repository.\\n- [Update this FAQ](https://github.com/DataTalksClub/faq/).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/035_721f9e0c29_how-can-we-contribute-to-the-course.md'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Function Calling with OpenAI\n",
        "\n",
        "# Let's create an agent now. In OpenAI's terminology, we'll need to use \"function calling\" (https://platform.openai.com/docs/guides/function-calling)\n",
        "\n",
        "# We will begin with our FAQ example and text search. You can easily extend it to vector or hybrid search or change it to the Evidently docs.\n",
        "\n",
        "# This is the function we implemented yesterday:\n",
        "\n",
        "def text_search(query):\n",
        "    return faq_index.search(query, num_results=5)\n",
        "\n",
        "# We can't just pass this function to OpenAI. First, we need to describe this function, so the LLM understands how to use it.\n",
        "\n",
        "# This is done using a special description format:\n",
        "\n",
        "text_search_tool = {\n",
        "    \"type\": \"function\",\n",
        "    \"name\": \"text_search\",\n",
        "    \"description\": \"Search the FAQ database\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"query\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"query\"],\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# This description tells OpenAI:\n",
        "# - The function is called text_search\n",
        "# - It searches the FAQ database\n",
        "# - It takes one required parameter: query (a string)\n",
        "# - The query should be the search text to look up in the course FAQ\n",
        "\n",
        "# Now we can use it\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant for a course\n",
        "\"\"\"\n",
        "\n",
        "question = \"I just discovered the course, can I join now?\"\n",
        "\n",
        "chat_messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "response = openai_client.responses.create(\n",
        "    model='openai/gpt-oss-20b',\n",
        "    input=chat_messages,\n",
        "    tools=[text_search_tool]\n",
        ")\n",
        "\n",
        "# Previously, we had a simple text response; now, the response includes function calls.\n",
        "# Let's look at response.output. In my case, it contains the following:\n",
        "\n",
        "print(response.output)\n",
        "\n",
        "# The agent analyzed the user's question and determined that to answer it, it needs to invoke the text_search function with the arguments {\"query\":\"join course\"}.\n",
        "\n",
        "# Let's invoke the function with these arguments:\n",
        "\n",
        "import json\n",
        "\n",
        "call = response.output[1]\n",
        "\n",
        "arguments = json.loads(call.arguments)\n",
        "result = text_search(**arguments)\n",
        "\n",
        "call_output = {\n",
        "    \"type\": \"function_call_output\",\n",
        "    \"call_id\": call.call_id,\n",
        "    \"output\": json.dumps(result),\n",
        "}\n",
        "\n",
        "# Here's what's happening:\n",
        "# - The LLM decided to execute a function and let us know about it\n",
        "# - We executed the function and saved the results\n",
        "# - Now we need to pass this information back to the LLM\n",
        "\n",
        "# We do it by extending the chat_messages list and sending the entire conversation history back to the LLM:\n",
        "\n",
        "chat_messages.append(call)\n",
        "chat_messages.append(call_output)\n",
        "\n",
        "response = openai_client.responses.create(\n",
        "    model='openai/gpt-oss-20b',\n",
        "    input=chat_messages,\n",
        "    tools=[text_search_tool]\n",
        ")\n",
        "\n",
        "print(response.output_text)\n",
        "\n",
        "# LLMs are stateless. When we make one call to the OpenAI API and then shortly afterwards make another, it doesn't know anything about the first call. So if we only send it call_output, it would have no idea how to respond to it.\n",
        "\n",
        "# This is why we need to send it the entire conversation history. It needs to know everything that happened so far:\n",
        "# - The system prompt (so it knows what the initial instructions are) - system_prompt\n",
        "# - The user prompt (so it knows what task it needs to perform) - question\n",
        "# - The decision to invoke the text_search tool (so it knows what function was called) - that's our call\n",
        "# - The output of the function (so it knows what the function returned) - that's our call_output\n",
        "\n",
        "\n",
        "# After we invoke it, we get back the response:\n",
        "#      “Yes, you can still join the course even after the start date. While you won't be able to officially register, you are eligible to submit your homework. Just keep in mind that there are deadlines\n",
        "#       for submitting assignments and final projects, so it's best not to leave everything to the last minute.”\n",
        "# This is a useful response that we were hoping to get.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUz_5VWAyldC",
        "outputId": "499ee385-970c-4c64-a349-da6d097d690d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResponseReasoningItem(id='resp_01k69t1s81fsd81m2q30bv7da7', summary=[], type='reasoning', content=[Content(text='The user asks: \"I just discovered the course, can I join now?\" We need to answer. We might need to search FAQ for relevant answer. Use text_search. The query: \"join now after discovering course\" or \"can I join course now\" etc. Let\\'s use search.', type='reasoning_text')], encrypted_content=None, status='completed'), ResponseFunctionToolCall(arguments='{\"query\":\"join course now\"}', call_id='fc_e047c4b9-6933-4e26-8f4f-b2637339804a', name='text_search', type='function_call', id='fc_e047c4b9-6933-4e26-8f4f-b2637339804a', status='completed')]\n",
            "Absolutely! You can still join the Data Engineering Zoomcamp – even after the cohort has started.\n",
            "\n",
            "### How to get in\n",
            "\n",
            "| Step | What to do | Link |\n",
            "|------|------------|------|\n",
            "| 1️⃣ | **Register** (the official sign‑up form). Even if you’re signing up after the start date, you’ll still receive all the learning material and can submit the assignments. | [Register here](https://airtable.com/shr6oVXeQvSI5HuWD) |\n",
            "| 2️⃣ | **Join the community** – the course Telegram channel is the best place to hear real‑time announcements, ask questions, and stay updated on deadlines. | [Telegram channel](https://t.me/dezoomcamp) |\n",
            "| 3️⃣ | **Set up your workspace** – install the required tools (Python 3, Google Cloud SDK, Terraform, Git, etc.). The FAQ “What can I do before the course starts?” has a handy checklist. | – |\n",
            "\n",
            "### Key points to keep in mind\n",
            "\n",
            "* **Deadlines still matter** – even if you’re joining late, you’re expected to submit the homework and capstone by the same dates as everyone else. Don’t wait until the last minute!  \n",
            "* **Prerequisites** – Basic coding, SQL, and some Python help are recommended but not required. If you’re unsure, review the *Prerequisites* FAQ for guidance.  \n",
            "* **Course content is evergreen** – All lecture videos, slides, and notebooks remain available after the cohort ends, so you can keep reviewing or even start your own pace.\n",
            "\n",
            "> **FAQ reference:**  \n",
            "> *“Course: Can I still join the course after the start date?”* – Yes, you can. Just be sure to keep up with the deadlines.\n",
            "\n",
            "If you need help setting up your environment or have any other questions, feel free to ask! Happy learning 🚀\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. System Prompt: Instructions\n",
        "\n",
        "# Let's take another look at the code we wrote previously\n",
        "\n",
        "chat_messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "response = openai_client.responses.create(\n",
        "    model='openai/gpt-oss-20b',\n",
        "    input=chat_messages,\n",
        "    tools=[text_search_tool]\n",
        ")\n",
        "\n",
        "# We have two things here:\n",
        "# - system_prompt contains instructions for the LLM\n",
        "# - question (\"user prompt\") is the actual question or task\n",
        "\n",
        "# The system prompt is very important: it influences how the agent behaves. This is how we can control what the agent does and how it responds to user questions.\n",
        "\n",
        "# Usually, the more complete the instructions in the system prompt are, the better the results.\n",
        "\n",
        "# So we can extend it\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant for a course.\n",
        "\n",
        "Use the search tool to find relevant information from the course materials before answering questions.\n",
        "\n",
        "If you can find specific information through search, use it to provide accurate answers.\n",
        "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
        "\"\"\"\n",
        "\n",
        "# When working with agents, the system prompt becomes one of the most essential variables we can adjust to influence our agent.\n",
        "# For example, if we want the agent to make multiple search queries, we can modify the prompt:\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant for a course.\n",
        "\n",
        "Always search for relevant information before answering.\n",
        "If the first search doesn't give you enough information, try different search terms.\n",
        "\n",
        "Make multiple searches if needed to provide comprehensive answers.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "SQsDXn6A2pGt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Pydantic AI\n",
        "\n",
        "# Dealing with function calls can be cumbersome. We first need to understand which function we need to invoke. Then we need to pass the results back to the LLM and perform other tasks. It's easy to make a mistake there.\n",
        "\n",
        "# That's why we'll use a library to handle it. There are many agentic libraries: OpenAI Agents SDK, Langchain, Pydantic AI, and many more.\n",
        "\n",
        "# Today, we will use Pydantic AI. I like its API; it's simpler than other libraries and has good documentation.\n",
        "\n",
        "# Let's install it\n",
        "\n",
        "!uv pip install pydantic-ai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H32UaSId-7HT",
        "outputId": "d6d71afb-e675-4b88-cb75-35c6dfce929e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 127ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Pydantic AI (and for other agents libraries), we don't need to describe the function in the JSON format like we did witht the plain OpenAI API. The libraries take care of it.\n",
        "\n",
        "# But we do need to add docstrings and type hints to our function. I asked ChatGPT to do it:\n",
        "\n",
        "from typing import List, Any\n",
        "\n",
        "def text_search(query: str) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Perform a text-based search on the FAQ index.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query string\n",
        "\n",
        "    Returns:\n",
        "        List[Any]: A list of up to 5 search results returned by the FAQ index\n",
        "    \"\"\"\n",
        "\n",
        "    return faq_index.search(query, num_results=5)"
      ],
      "metadata": {
        "id": "YvSkVIJEm6hC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"faq_agent\",\n",
        "    instructions=system_prompt,\n",
        "    tools=[text_search],\n",
        "    model='gpt-4o-mini'\n",
        ")\n",
        "\n",
        "\n",
        "# We don't need to do anything with our text_search function. We just pass it directly to the agent.\n",
        "# Let's run it:\n",
        "\n",
        "question = \"I just discovered the course, can I join now?\"\n",
        "\n",
        "result = await agent.run(user_prompt=question)"
      ],
      "metadata": {
        "id": "XTfkVHmPnq3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8321330a"
      },
      "source": [
        "# We use await because Pydantic AI is asynchronous. If you're not running in Jupyter, you need to use asyncio.run():\n",
        "\n",
        "import asyncio\n",
        "\n",
        "result = asyncio.run(agent.run(user_prompt=question))\n",
        "\n",
        "# The output:\n",
        "# “Yes, you can still join the course even after the start date. Although you may not officially register, you are eligible to submit your homework. Just keep in mind that there are deadlines for turning in homework and final projects, so it's advisable not to delay everything until the last minute.”\n",
        "# We can also look inside the result to get a detailed breakdown of the agent's reasoning and actions:\n",
        "result.new_messages()\n",
        "\n",
        "# It contains four items:\n",
        "# - ModelRequest: Represents a request sent to the model. It includes the user's prompt (UserPromptPart) and the agent's instructions.\n",
        "# - ModelResponse: The model's reply. We see a ToolCallPart with the decision to invoke text_search.\n",
        "# - ModelRequest: Contains ToolReturnPart - the results returned by the tool (search results from the FAQ index).\n",
        "# - ModelResponse: The final answer generated by the model in TextPart.\n",
        "\n",
        "# Pydantic AI and other frameworks handle all the complexity of function calling for us. We don't need to manually parse responses, handle tool calls, or manage conversation history. This makes our code cleaner and less error-prone.\n",
        "\n",
        "# We implemented an agent. Great! But how good is it? Is the prompt we came up good? What's better for our agent, text search, vector search or hybrid? Tomorrow we will be able to answer these questions: we will learn how to use AI to evaluate our agent.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cCvzM_7qn_Lo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}