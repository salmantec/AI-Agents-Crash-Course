{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFP06kQoAXsfjDltY60yZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmantec/AI-Agents-Crash-Course/blob/feat%2FDay-2/Day-2/Day_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Chunking and Intelligent Processing for Data"
      ],
      "metadata": {
        "id": "25BnhaywWe1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In the first part of the course, we focus on data preparation – the process of properly preparing data before it can be used for AI agents.\n",
        "\n",
        "# Small and Large Documents:\n",
        "\n",
        "# Yesterday, we prepared data from Github repo. For small sources, like FAQs, that is sufficient.\n",
        "# The questions and answers are small enough. We can put them directly into the search engine\n",
        "\n",
        "# But large documents, like Evidently's docs, Let's take a look at this one: https://github.com/evidentlyai/docs/blob/main/docs/library/descriptors.mdx. can cause problems when passed directly to an LLM:\n",
        "\n",
        "# Why We Need to Prepare Large Documents Before Using Them\n",
        "\n",
        "# Large documents create several problems:\n",
        "\n",
        "# Token limits: Most LLMs have maximum input token limits\n",
        "# Cost: Longer prompts cost more money\n",
        "# Performance: LLMs perform worse with very long contexts\n",
        "# Relevance: Not all parts of a long document are relevant to a specific question\n",
        "\n",
        "# So we need to split documents into smaller subdocuments. For AI applications like RAG (which we will discuss tomorrow), this process is referred to as \"chunking.\"\n",
        "\n",
        "# 'Chunking': breaking long documents into smaller, focused pieces that are easier (and cheaper) for AI to process"
      ],
      "metadata": {
        "id": "FXyBIwHTWcRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Today's task:\n",
        "\n",
        "# Explore 3 different chunking methods:\n",
        "# - Simple sliding window: cut into overlapping chunks (Simple character-based chunking)\n",
        "# - Paragraph and section splits: use natural document structure (Paragraph and section-based chunking)\n",
        "# - LLM-powered chunking: Intelligent, semantic splits (requires OpenAI or Groq account) (Intelligent chunking with LLM)\n",
        "\n",
        "# Just so you know, for the last section, you will need an OpenAI account or an account from an alternative LLM provider such as Groq."
      ],
      "metadata": {
        "id": "pXD9FdH1WcNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS7Lf6v9Vr_I",
        "outputId": "6334d524-db6c-4fe1-f0e7-e5c7af5d37b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uv\n",
            "  Downloading uv-0.8.22-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.8.22-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.8.22\n"
          ]
        }
      ],
      "source": [
        "!pip install uv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install requests python-frontmatter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoTQU4pRVwCC",
        "outputId": "ff6c6851-f609-4609-849b-4dcb88538c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m7 packages\u001b[0m \u001b[2min 148ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-frontmatter\u001b[0m\u001b[2m==1.1.0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Simple chunking\n",
        "\n",
        "# Let's start with a simple chunking. This will be sufficient for most cases."
      ],
      "metadata": {
        "id": "tet3mrKTWvbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can continue with the notebook from Day 1. We already downloaded the data from Evidently docs. We put them into the evidently_docs list.\n",
        "\n",
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import frontmatter\n",
        "\n",
        "def read_repo_data(repo_owner, repo_name):\n",
        "  \"\"\"\n",
        "  Download and parse all markdown files from a github repository\n",
        "\n",
        "  Args:\n",
        "    repo_owner : Github username or organization\n",
        "    repo_name: Repository name\n",
        "\n",
        "  Returns:\n",
        "    List of dictionaries containing file content and metadata\n",
        "  \"\"\"\n",
        "  prefix = 'https://codeload.github.com'\n",
        "  url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
        "  resp = requests.get(url)\n",
        "\n",
        "  if resp.status_code != 200:\n",
        "    raise Exception(f\"Failed to download repository {repo_owner}/{repo_name}: {resp.status_code}\")\n",
        "\n",
        "  repository_data = []\n",
        "\n",
        "  # Create a ZipFile object from the downloaded content\n",
        "  zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
        "\n",
        "  for file_info in zf.infolist():\n",
        "    filename = file_info.filename\n",
        "    filename_lower = filename.lower()\n",
        "\n",
        "    if not (filename_lower.endswith('.md') or (filename_lower.endswith('.mdx'))):\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      with zf.open(file_info) as f_in:\n",
        "        content = f_in.read().decode('utf-8', errors='ignore')\n",
        "        post = frontmatter.loads(content)\n",
        "        data = post.to_dict()\n",
        "        data['filename'] = filename\n",
        "        repository_data.append(data)\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing {filename}: {e}\")\n",
        "      continue\n",
        "\n",
        "  zf.close()\n",
        "  return repository_data"
      ],
      "metadata": {
        "id": "QgW4sdFKVv9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
        "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
        "\n",
        "print(f\"FAQ Documents: {len(dtc_faq)}\")\n",
        "print(f\"Evidently Docs: {len(evidently_docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlsTHlICVv53",
        "outputId": "ffc58675-cd85-4dca-e5eb-7c4174a45d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAQ Documents: 1217\n",
            "Evidently Docs: 95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how the document at index 45 looks like:\n",
        "\n",
        "len(evidently_docs[45]['content'])\n",
        "\n",
        "\n",
        "# The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. For example, for size of 2000 characters, we will have:\n",
        "\n",
        "# Chunk 1: 0..2000\n",
        "# Chunk 2: 2000..4000\n",
        "# Chunk 3: 4000..6000\n",
        "\n",
        "# And so on.\n",
        "\n",
        "# However, this approach has disadvantages:\n",
        "\n",
        "# Context loss: Important information might be split in the middle\n",
        "# Incomplete sentences: Chunks might end mid-sentence\n",
        "# Missing connections: Related information might end up in different chunks\n",
        "\n",
        "# That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
        "\n",
        "# Chunk 1: 0..2000\n",
        "# Chunk 2: 1000..3000\n",
        "# Chunk 3: 2000..4000\n",
        "# ...\n",
        "\n",
        "# This is better for AI because:\n",
        "\n",
        "# Continuity: Important information isn't lost at chunk boundaries\n",
        "# Context preservation: Related sentences stay together in at least one chunk\n",
        "# Better search: Queries can match information even if it spans chunk boundaries\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuEjtToOW3Jy",
        "outputId": "b28a5dcf-1bbf-43bf-ba38-18a9af35f97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21712"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This approach is known as the \"sliding window\" method. This is how we implement it in Python:\n",
        "\n",
        "def sliding_window(sequence, size, step):\n",
        "  if size <= 0 or step <= 0:\n",
        "    raise ValueError(\"Size and step must be positive\")\n",
        "\n",
        "  n = len(sequence)\n",
        "  result = []\n",
        "  for i in range(0, n, step):\n",
        "    chunk = sequence[i:i+size]\n",
        "    result.append({'start': i, 'chunk': chunk})\n",
        "    if i + size >= n:\n",
        "      break\n",
        "\n",
        "  return result\n",
        "\n",
        "# Let's apply it for document 45. This gives us 21 chunks:\n",
        "\n",
        "# 0..2000\n",
        "# 1000..3000\n",
        "# ...\n",
        "# 19000..21000\n",
        "# 20000..21712"
      ],
      "metadata": {
        "id": "O3Rlh5bzW3HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's process all the documents\n",
        "\n",
        "evidently_chunks = []\n",
        "\n",
        "for doc in evidently_docs:\n",
        "  doc_copy = doc.copy()\n",
        "  doc_content = doc_copy.pop('content')\n",
        "  chunks = sliding_window(doc_content, 2000, 1000)\n",
        "  for chunk in chunks:\n",
        "    chunk.update(doc_copy)\n",
        "  evidently_chunks.extend(chunks)\n",
        "\n",
        "\n",
        "# Note that we use copy() and pop() operations:\n",
        "\n",
        "# doc.copy() creates a shallow copy of the document dictionary\n",
        "# doc_copy.pop('content') removes the 'content' key and returns its value\n",
        "# This way we preserve the original dictionary keys that we can use later in the chunks.\n",
        "\n",
        "# This way, we obtain 575 chunks from 95 documents\n",
        "\n",
        "len(evidently_chunks)\n",
        "\n",
        "# We can play with the parameters by including more or less content. 2000 characters is usually good enough for RAG applications.\n",
        "\n",
        "# There are some alternative approaches:\n",
        "\n",
        "# - Token-based chunking: You first tokenize the content (turn it into a sequence of words) and then do a sliding window over tokens\n",
        "#   - Advantages: More precise control over LLM input size\n",
        "#   - Disadvantages: Doesn't work well for documents with code\n",
        "# - Paragraph splitting: Split by paragraphs\n",
        "# - Section splitting: Split by sections\n",
        "# - AI-powered splitting: Let AI split the text intelligently\n",
        "\n",
        "# We won't cover token-based chunking here, as we're working with documents that contain code. But it's easy to implement - ask ChatGPT for help if you need it for text-only content.\n"
      ],
      "metadata": {
        "id": "_iJj9ZlFW2-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "idU9xosaeS5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Splitting by Paragraphs and sections"
      ],
      "metadata": {
        "id": "o447U4tVnGJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting by paragraphs is relatively easy:\n",
        "\n",
        "import re\n",
        "text = evidently_docs[45]['content']\n",
        "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
        "\n",
        "# paragraphs[0:3]\n",
        "# We use \\n\\s*\\n regex pattern for splitting:\n",
        "\n",
        "# - \\n matches a newline\n",
        "# - \\s* matches zero or more whitespace characters\n",
        "# - \\n matches another newline\n",
        "# So \\n\\s*\\n matches two newlines with optional whitespace between them\n",
        "\n",
        "# This works well for literature, but it doesn't work well for documents. Most paragraphs in technical documentation are very short.\n"
      ],
      "metadata": {
        "id": "PrMNpsMDnGGi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now looks at section splitting. Here, we take advantage of the documents' structure. markdown documents have this structure:\n",
        "\n",
        "# Heading 1\n",
        "## Heading 2\n",
        "### Heading 3\n",
        "\n",
        "# What we can do is split by headers\n",
        "\n",
        "# For that we will use regex too:\n",
        "\n",
        "import re\n",
        "\n",
        "def split_markdown_by_level(text, level=2):\n",
        "  \"\"\"\n",
        "  Split markdown text by a specific header level.\n",
        "\n",
        "  :param text: Markdown text as a string\n",
        "  :param level: Header level to split on\n",
        "  :return: List of sections as strings\n",
        "  \"\"\"\n",
        "  # This regex matches markdown headers\n",
        "  # For level 2, it matches lines starting with \"## \"\n",
        "\n",
        "  header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
        "  pattern = re.compile(header_pattern, re.MULTILINE)\n",
        "\n",
        "  # Split and keep the headers\n",
        "  parts = pattern.split(text)\n",
        "\n",
        "  sections = []\n",
        "  for i in range(1, len(parts), 3):\n",
        "    # We step by 3 because regex.split() with capturing groups returns:\n",
        "    # [before_match, group1, group2, after_match, ...]\n",
        "    # here group1 is \"## \", group2 is the header text\n",
        "    header = parts[i] + parts[i + 1] # \"## \" + \"Title\"\n",
        "    header = header.strip()\n",
        "\n",
        "    # Get the content after this header\n",
        "    content = \"\"\n",
        "    if i+2 < len(parts):\n",
        "      content = parts[i+2].strip()\n",
        "\n",
        "    if content:\n",
        "      section = f'{header}\\n\\n{content}'\n",
        "    else:\n",
        "      section = header\n",
        "    sections.append(section)\n",
        "\n",
        "  return sections\n",
        "\n",
        "# Note: This code may not work perfectly if we want to split by level 1 headings and have Python code with # comments. But in general, this is not a big problem for documentation."
      ],
      "metadata": {
        "id": "9dKpEgUHnFe9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we iterate over all the docs to create the final result:\n",
        "\n",
        "evidently_chunks = []\n",
        "\n",
        "for doc in evidently_docs:\n",
        "  doc_copy = doc.copy()\n",
        "  doc_content = doc_copy.pop('content')\n",
        "  sections = split_markdown_by_level(doc_content, level = 2)\n",
        "  for section in sections:\n",
        "    section_doc = doc_copy.copy()\n",
        "    section_doc['section'] = section\n",
        "    evidently_chunks.append(section_doc)\n",
        "\n",
        "# Like previously, copy() creates a copy of the document metadata. pop('content') removes and returns the content. This way, each section gets the same metadata (title, description) as the original document.\n",
        "\n",
        "# This was more intelligent processing, but we can go even further and use LLMs for that.\n"
      ],
      "metadata": {
        "id": "XwauY8OLpRRM"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}