{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd1QwyJkqN9GvNTByfyGQo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmantec/AI-Agents-Crash-Course/blob/feat%2FDay-5/Day-5/Day_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "pYsKTChi_ebf"
      },
      "outputs": [],
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Welcome to day five of our AI Agents Crash Course.\n",
        "\n",
        "# Yesterday we learned about function calling and created our first agent using Pydantic AI.\n",
        "# But is this agent actually good? Today we will see how to answer this question.\n",
        "\n",
        "# In particular, we will cover:\n",
        "# - Build a logging system to track agent interactions\n",
        "# - Create automated evaluation using AI as a judge\n",
        "# - Generate test data automatically\n",
        "# - Measure agent performance with metrics\n",
        "\n",
        "# At the end of this lesson, you'll have a thoroughly tested agent with performance metrics.\n",
        "\n",
        "# In this lesson, we'll use the FAQ database with text search, but it's applicable for any other use case.\n",
        "\n",
        "# This is going to be a long lesson, but an important one. Evaluation is critical for building reliable AI systems. Without proper evaluation, you can't tell if your changes improve or hurt performance. You can't compare different approaches. And you can't build confidence before deploying to users.\n",
        "\n",
        "# So let's start!\n"
      ],
      "metadata": {
        "id": "ymROteaYBUOk"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging\n",
        "\n",
        "# The easiest thing we can do to evaluate an agent is interact with it. We ask something and look at the response. Does it make sense? For most cases, it should.\n",
        "\n",
        "# \"Vibe check\" - we interact with it, and if we like the results, we go ahead and deploy it.\n",
        "\n",
        "# If we don't like something, we go back and change things:\n",
        "# - Maybe our chunking method is not suitable? Maybe we need to have a bigger window size?\n",
        "# - Is our system prompt good? Maybe we need more precise instructions?\n",
        "# - Or we want to change something else\n",
        "# And we iterate.\n",
        "\n",
        "# It might be okay for the first MVP, but how can we make sure the result at the end is actually good?\n",
        "\n",
        "# We need systematic evaluation. Manual testing doesn't scale - you can't manually test every possible input and scenario. With systematic evaluation, we can test hundreds or thousands of cases automatically.\n",
        "\n",
        "# We also need to base our decisions on data. It will help us to\n",
        "# - Compare different approaches\n",
        "# - Track improvements\n",
        "# - Identify edge cases\n",
        "\n",
        "# We can start collecting this data ourselves: start with vibe checking, but be smart about it. We don't just test it, but also record the results.\n"
      ],
      "metadata": {
        "id": "g8-TWGpFBUL4"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uv"
      ],
      "metadata": {
        "id": "LlaJpjBUDySx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "644333e1-71be-42e9-b9a6-1cd69841ff70"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uv in /usr/local/lib/python3.12/dist-packages (0.8.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install openai minsearch requests python-frontmatter pydantic-ai"
      ],
      "metadata": {
        "id": "uFBFGlzUDzn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf86c56-7c68-4122-89a5-68c72902ac61"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 112ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "UlV0_74hjuFa"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  find read_repo_data in the first lesson and sliding_window in the second lesson\n",
        "\n",
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import frontmatter\n",
        "\n",
        "def read_repo_data(repo_owner, repo_name):\n",
        "  \"\"\"\n",
        "  Download and parse all markdown files from a github repository\n",
        "\n",
        "  Args:\n",
        "    repo_owner : Github username or organization\n",
        "    repo_name: Repository name\n",
        "\n",
        "  Returns:\n",
        "    List of dictionaries containing file content and metadata\n",
        "  \"\"\"\n",
        "  prefix = 'https://codeload.github.com'\n",
        "  url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
        "  resp = requests.get(url)\n",
        "\n",
        "  if resp.status_code != 200:\n",
        "    raise Exception(f\"Failed to download repository {repo_owner}/{repo_name}: {resp.status_code}\")\n",
        "\n",
        "  repository_data = []\n",
        "\n",
        "  # Create a ZipFile object from the downloaded content\n",
        "  zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
        "\n",
        "  for file_info in zf.infolist():\n",
        "    filename = file_info.filename\n",
        "    filename_lower = filename.lower()\n",
        "\n",
        "    if not (filename_lower.endswith('.md') or (filename_lower.endswith('.mdx'))):\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      with zf.open(file_info) as f_in:\n",
        "        content = f_in.read().decode('utf-8', errors='ignore')\n",
        "        post = frontmatter.loads(content)\n",
        "        data = post.to_dict()\n",
        "        data['filename'] = filename\n",
        "        repository_data.append(data)\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing {filename}: {e}\")\n",
        "      continue\n",
        "\n",
        "  zf.close()\n",
        "  return repository_data"
      ],
      "metadata": {
        "id": "guINHuckDsx5"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now index this data with minsearch:\n",
        "\n",
        "from minsearch import Index\n",
        "\n",
        "# For DataTalksClub FAQ, it's similar, except we don't need to chunk the data. For the data engineering course, it'll look like this:\n",
        "\n",
        "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
        "\n",
        "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
        "\n",
        "faq_index = Index(\n",
        "    text_fields=[\"question\", \"content\"],\n",
        "    keyword_fields=[]\n",
        ")\n",
        "\n",
        "faq_index.fit(de_dtc_faq)\n",
        "\n",
        "query = 'Course: Can I still join the course after the start date?'\n",
        "results = faq_index.search(query)\n",
        "print(results)\n",
        "\n",
        "# This is text search, also known as \"lexical search\". We look for exact matches between our query and the documents."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMgXZTOCDn9W",
        "outputId": "156ab087-70a9-46e9-99bd-b978b1aa6187"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '33fc260cd8', 'question': 'Course: What can I do before the course starts?', 'sort_order': 5, 'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'}, {'id': 'bfafa427b3', 'question': 'Course: What are the prerequisites for this course?', 'sort_order': 2, 'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'}, {'id': '900f60fd25', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'sort_order': 15, 'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}, {'id': '4dbd2eea47', 'question': 'Homework: Are late submissions of homework allowed?', 'sort_order': 17, 'content': 'No, late submissions are not allowed. However, if the form is still open after the due date, you can still submit the homework. Confirm your submission by checking the date-timestamp on the Course page. Ensure you are logged in.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/017_4dbd2eea47_homework-are-late-submissions-of-homework-allowed.md'}, {'id': '52217fc51b', 'question': 'Course: I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'sort_order': 4, 'content': \"You don't need a confirmation email. You're accepted. You can start learning and submitting homework without registering. Registration was just to gauge interest before the start date.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/004_52217fc51b_course-i-have-registered-for-the-data-engineering.md'}, {'id': '721f9e0c29', 'question': 'How can we contribute to the course?', 'sort_order': 35, 'content': '- [Star the repository](https://github.com/DataTalksClub/data-engineering-zoomcamp).\\n- Share it with friends if you find it useful.\\n- Create a pull request (PR) if you can improve the text or structure of the repository.\\n- [Update this FAQ](https://github.com/DataTalksClub/faq/).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/035_721f9e0c29_how-can-we-contribute-to-the-course.md'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's the agent we created yesterday:\n",
        "\n",
        "from typing import List, Any\n",
        "from pydantic_ai import Agent\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "def text_search(query: str) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Perform a text-based search on the FAQ index.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query string.\n",
        "\n",
        "    Returns:\n",
        "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
        "    \"\"\"\n",
        "    return faq_index.search(query, num_results=5)\n",
        "\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant for a  course.\n",
        "\n",
        "Use the search tool to find relevant information from the course materials before answering questions.\n",
        "\n",
        "If you can find specific information through search, use it to provide accurate answers.\n",
        "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Get API keys from Colab secrets\n",
        "groq_api_key = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# Set environment variables (pydantic-ai can also read from these)\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"faq_agent\",\n",
        "    instructions=system_prompt,\n",
        "    tools=[text_search],\n",
        "    model='groq:gemma2-9b-it'\n",
        ")\n",
        "\n",
        "question = \"how do I install Kafka in Python?\"\n",
        "result = await agent.run(user_prompt=question)\n",
        "\n",
        "print(result.output)\n",
        "\n",
        "\n",
        "# Here's what we want to record:\n",
        "# - The system prompt that we used\n",
        "# - The model\n",
        "# - The user query\n",
        "# - The tools we use\n",
        "# - The responses and the back-and-forth interactions between the LLM and our tools\n",
        "# - The final response\n",
        "\n",
        "# To make it simpler, we'll implement a simple logging system ourselves: we will just write logs to json files.\n",
        "\n",
        "# You shouldn't use it in production. In practice, you will want to send these logs to some log collection system, or use specialized LLM evaluation tools like Evidently, LangWatch or Arize Phoenix.\n"
      ],
      "metadata": {
        "id": "qX9vecjYBUJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's extract all this information from the agent and from the run results:\n",
        "\n",
        "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
        "\n",
        "\n",
        "def log_entry(agent, messages, source=\"user\"):\n",
        "    tools = []\n",
        "\n",
        "    for ts in agent.toolsets:\n",
        "        tools.extend(ts.tools.keys())\n",
        "\n",
        "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
        "\n",
        "    return {\n",
        "        \"agent_name\": agent.name,\n",
        "        \"system_prompt\": agent._instructions,\n",
        "        \"provider\": agent.model.system,\n",
        "        \"model\": agent.model.model_name,\n",
        "        \"tools\": tools,\n",
        "        \"messages\": dict_messages,\n",
        "        \"source\": source\n",
        "    }\n",
        "\n",
        "\n",
        "# This code extracts the key information from our agent:\n",
        "# - the configuration (name, prompt, model)\n",
        "# - available tools\n",
        "# - complete message history (user input, tool calls, responses)\n",
        "\n",
        "# We also use ModelMessagesTypeAdapter.dump_python(messages) to convert internal message format into regular Python dictionaries. This makes it easier to save it to JSON and process later.\n",
        "\n",
        "# We also add the source parameter. It tracks where the question came from. We start with \"user\" but later we'll use AI-generated queries. Sometimes it may be important to tell them apart for analysis.0\n",
        "\n",
        "# This code is generic so it will work with any Pydantic AI agent. If you use a different library, you'll need to adjust this code."
      ],
      "metadata": {
        "id": "JUcYqHaDBUHY"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's write these logs to a folder:\n",
        "\n",
        "import json\n",
        "import secrets\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "LOG_DIR = Path('logs')\n",
        "LOG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "def serializer(obj):\n",
        "    if isinstance(obj, datetime):\n",
        "        return obj.isoformat()\n",
        "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
        "\n",
        "\n",
        "def log_interaction_to_file(agent, messages, source='user'):\n",
        "    entry = log_entry(agent, messages, source)\n",
        "\n",
        "    ts = entry['messages'][-1]['timestamp']\n",
        "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    rand_hex = secrets.token_hex(3)\n",
        "\n",
        "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
        "    filepath = LOG_DIR / filename\n",
        "\n",
        "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
        "        json.dump(entry, f_out, indent=2, default=serializer)\n",
        "\n",
        "    return filepath\n",
        "\n",
        "  # This code:\n",
        "  # - Creates a logs directory (if not created previously)\n",
        "  # - Generates unique filenames with timestamp and random hex\n",
        "  # - Saves complete interaction logs as JSON files\n",
        "  # - Handles datetime serialization (using the serialized function)"
      ],
      "metadata": {
        "id": "UoiJjDRxShYj"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can interact with it and do some vibe checking:\n",
        "question = input()\n",
        "result = await agent.run(user_prompt=question)\n",
        "print(result.output)\n",
        "log_interaction_to_file(agent, result.new_messages())\n",
        "\n",
        "# This creates a simple interactive loop where:\n",
        "# User enters a question\n",
        "# Agent processes it and responds\n",
        "# Complete interaction is logged to a file\n",
        "\n",
        "# Try these questions:\n",
        "# - how do I use docker on windows?\n",
        "# - can I join late and get a certificate?\n",
        "# - what do I need to do for the certificate?\n"
      ],
      "metadata": {
        "id": "7T40CD_fShVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6_JdH9DShRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding References\n",
        "\n",
        "# When interacting with the agent, I noticed one thing: it doesn't include the reference to the original documents.\n",
        "\n",
        "# Let's fix it by adjusting the prompt\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant for a course.\n",
        "\n",
        "Use the search tool to find relevant information from the course materials before answering questions.\n",
        "\n",
        "If you can find specific information through search, use it to provide accurate answers.\n",
        "\n",
        "Always include references by citing the filename of the source material you used.\n",
        "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
        "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
        "\n",
        "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Create another version of agent, let's call it faq_agent_v2\n",
        "agent = Agent(\n",
        "    name=\"faq_agent_v2\",\n",
        "    instructions=system_prompt,\n",
        "    tools=[text_search],\n",
        "    model='groq:gemma2-9b-it'\n",
        ")\n",
        "\n",
        "# Now we can interact with it and do some vibe checking:\n",
        "question = input()\n",
        "result = await agent.run(user_prompt=question)\n",
        "print(result.output)\n",
        "log_interaction_to_file(agent, result.new_messages())\n",
        "\n",
        "# question - can I join late and get a certificate?"
      ],
      "metadata": {
        "id": "KKraDY3c4FqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kSbCCdC54Fiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM as a Judge\n",
        "\n",
        "# You can ask your colleagues to also do a \"vibe check\", but make sure you record the data. Often collecting 10-20 examples and manually inspecting them is enough to understand how your model is doing.\n",
        "\n",
        "# Don't be afraid of putting manual work into evaluation. Manual evaluation will help you understand edge cases, learn what good responses look like and think of evaluation criteria for automated checks later.\n",
        "\n",
        "# For example, I manually inspected the output and noticed that references are missing. So we will later add it as one of the checks.\n",
        "\n",
        "# So, in our case, we can have the following checks:\n",
        "# - Does the agent follow the instructions?\n",
        "# - Given the question, does the answer make sense?\n",
        "# - Does it include references?\n",
        "# - Did the agent use the available tools?\n",
        "\n",
        "# We don't have to evaluate this manually. Instead, we can delegate this to AI. This technique is called \"LLM as a Judge\".\n",
        "\n",
        "# The idea is simple: we use one LLM to evaluate the outputs of another LLM. This works because LLMs are good at following detailed evaluation criteria.\n"
      ],
      "metadata": {
        "id": "R3UWyzOO5-dU"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our system prompt for the judge (we'll call it \"evaluation agent\" because it sounds cooler) can look like that:\n",
        "\n",
        "evaluation_prompt = \"\"\"\n",
        "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
        "We also include the entire log (<LOG>) for analysis.\n",
        "\n",
        "For each item, check if the condition is met.\n",
        "\n",
        "Checklist:\n",
        "\n",
        "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
        "- instructions_avoid: The agent avoided doing things it was told not to do\n",
        "- answer_relevant: The response directly addresses the user's question\n",
        "- answer_clear: The answer is clear and correct\n",
        "- answer_citations: The response includes proper citations or sources when required\n",
        "- completeness: The response is complete and covers all key aspects of the request\n",
        "- tool_call_search: Is the search tool invoked?\n",
        "\n",
        "Output true/false for each check and provide a short explanation for your judgment.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# Since we expect a very well defined structure of the response, we can use structured output.\n",
        "\n",
        "# We can define a Pydantic class with the expected response structure, and the LLM will produce output that matches this schema exactly.\n",
        "\n",
        "# This is how we do it:\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class EvaluationCheck(BaseModel):\n",
        "    check_name: str\n",
        "    justification: str\n",
        "    check_pass: bool\n",
        "\n",
        "class EvaluationChecklist(BaseModel):\n",
        "    checklist: list[EvaluationCheck]\n",
        "    summary: str\n",
        "\n",
        "\n",
        "# This code defines the structure we expect from our evaluation:\n",
        "# - Each check has a name, justification, and pass/fail result\n",
        "# - The overall evaluation includes a list of checks and a summary\n",
        "\n",
        "# Note that justification comes before check_pass. This makes the LLM reason about the answer before giving the final judgment, which typically leads to better evaluation quality.\n",
        "\n",
        "# With Pydantic AI in order to make the output follow the specified class, we use the parameter output_type:\n",
        "\n",
        "eval_agent = Agent(\n",
        "    name='eval_agent',\n",
        "    model='groq:llama-3.1-8b-instant',\n",
        "    instructions=evaluation_prompt,\n",
        "    output_type=EvaluationChecklist\n",
        ")\n",
        "\n",
        "\n",
        "# Usually it's a good idea to evaluate the results of one model (in our case, \"gpt-4o-mini\") with another model (e.g. \"gpt-5-nano\").\n",
        "# A different model can catch mistakes, reduce self-bias, and give a second opinion. This makes evaluations more reliable.\n",
        "\n",
        "# We have the instructions, and we have the agent. In order to run the agent, it needs input. We'll start with a template:\n",
        "\n",
        "user_prompt_format = \"\"\"\n",
        "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
        "<QUESTION>{question}</QUESTION>\n",
        "<ANSWER>{answer}</ANSWER>\n",
        "<LOG>{log}</LOG>\n",
        "\"\"\".strip()\n",
        "\n",
        "# We use XML markup because it's easier and more clear for LLMs to understand the input. XML tags help the model see the structure and boundaries of different sections in the prompt.\n",
        "\n",
        "# Let's fill it in. First, define a helper function for loading JSON log files:\n",
        "\n",
        "def load_log_file(log_file):\n",
        "    with open(log_file, 'r') as f_in:\n",
        "        log_data = json.load(f_in)\n",
        "        log_data['log_file'] = log_file\n",
        "        return log_data\n",
        "\n",
        "# We also add the filename in the result - it'll help us with tracking later.\n",
        "\n",
        "# Now let's use it:\n",
        "\n",
        "log_record = load_log_file('logs/faq_agent_v2_20251004_071817_8d1665.json')\n",
        "\n",
        "instructions = log_record['system_prompt']\n",
        "question = log_record['messages'][0]['parts'][0]['content']\n",
        "answer = log_record['messages'][-1]['parts'][0]['content']\n",
        "log = json.dumps(log_record['messages'])\n",
        "\n",
        "user_prompt = user_prompt_format.format(\n",
        "    instructions=instructions,\n",
        "    question=question,\n",
        "    answer=answer,\n",
        "    log=log\n",
        ")\n",
        "\n",
        "\n",
        "# The user input is ready and we can test it!\n",
        "\n",
        "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
        "\n",
        "checklist = result.output\n",
        "print(checklist.summary)\n",
        "\n",
        "for check in checklist.checklist:\n",
        "    print(check)\n",
        "\n",
        "\n",
        "# This code:\n",
        "# - Loads a saved interaction log\n",
        "# - Extracts the key components (instructions, question, answer, full log)\n",
        "# - Formats them into the evaluation prompt\n",
        "# - Runs the evaluation agent\n",
        "# - Prints the results\n",
        "\n"
      ],
      "metadata": {
        "id": "f7Ii2SbQ4Fbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that we're putting the entire conversation log into the prompt, which is not really necessary. We can reduce it to make it less verbose.\n",
        "\n",
        "# For example, like that:\n",
        "\n",
        "def simplify_log_messages(messages):\n",
        "    log_simplified = []\n",
        "\n",
        "    for m in messages:\n",
        "        parts = []\n",
        "\n",
        "        for original_part in m['parts']:\n",
        "            part = original_part.copy()\n",
        "            kind = part['part_kind']\n",
        "\n",
        "            if kind == 'user-prompt':\n",
        "                del part['timestamp']\n",
        "            if kind == 'tool-call':\n",
        "                del part['tool_call_id']\n",
        "            if kind == 'tool-return':\n",
        "                del part['tool_call_id']\n",
        "                del part['metadata']\n",
        "                del part['timestamp']\n",
        "                # Replace actual search results with placeholder to save tokens\n",
        "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
        "            if kind == 'text':\n",
        "                del part['id']\n",
        "\n",
        "            parts.append(part)\n",
        "\n",
        "        message = {\n",
        "            'kind': m['kind'],\n",
        "            'parts': parts\n",
        "        }\n",
        "\n",
        "        log_simplified.append(message)\n",
        "    return log_simplified\n",
        "\n",
        "# We make it simpler:\n",
        "# - remove timestamps and IDs that aren't needed for evaluation\n",
        "# - replace actual search results with a placeholder\n",
        "# - keep only the essential structure\n",
        "\n",
        "# This is helpful because it reduces the number of tokens we send to the evaluation model, which lowers the costs and speeds up evaluation."
      ],
      "metadata": {
        "id": "J2yuwzom8oYm"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's put everything together\n",
        "\n",
        "async def evaluate_log_record(eval_agent, log_record):\n",
        "    messages = log_record['messages']\n",
        "\n",
        "    instructions = log_record['system_prompt']\n",
        "    question = messages[0]['parts'][0]['content']\n",
        "    answer = messages[-1]['parts'][0]['content']\n",
        "\n",
        "    log_simplified = simplify_log_messages(messages)\n",
        "    log = json.dumps(log_simplified)\n",
        "\n",
        "    user_prompt = user_prompt_format.format(\n",
        "        instructions=instructions,\n",
        "        question=question,\n",
        "        answer=answer,\n",
        "        log=log\n",
        "    )\n",
        "\n",
        "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
        "    return result.output\n",
        "\n",
        "\n",
        "log_record = load_log_file('logs/faq_agent_v2_20251004_071817_8d1665.json')\n",
        "eval1 = await evaluate_log_record(eval_agent, log_record)\n",
        "\n",
        "print(eval1)\n",
        "\n",
        "# We know how to log our data and how to run evals on our logs.\n",
        "# Great. But how do we get more data to get a better understanding of the performance of our model?\n"
      ],
      "metadata": {
        "id": "CV3m_E008oVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_X2JvKK8oSx"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Generation:\n",
        "\n",
        "# WE can ask AI to help. What If we used it for generating more questions ? Let's do that.\n",
        "\n",
        "# We can sample some records from our database. Then for each record, ask an LLM to generate a question based on the record.\n",
        "# We use this question as input to our agent and log the answers.\n",
        "\n",
        "# Let's start by defining the question generator:\n",
        "\n",
        "question_generation_prompt = \"\"\"\n",
        "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
        "\n",
        "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
        "\n",
        "The questions should:\n",
        "\n",
        "- Be natural and varied in style\n",
        "- Range from simple to complex\n",
        "- Include both specific technical questions and general course questions\n",
        "\n",
        "Generate one question for each record.\n",
        "\"\"\".strip()\n",
        "\n",
        "class QuestionsList(BaseModel):\n",
        "    questions: list[str]\n",
        "\n",
        "question_generator = Agent(\n",
        "    name=\"question_generator\",\n",
        "    instructions=question_generation_prompt,\n",
        "    model='groq:llama-3.1-8b-instant',\n",
        "    output_type=QuestionsList\n",
        ")\n",
        "\n",
        "# This prompt is designed for our specific use case (data engineering course FAQ). You should adjust it for your project.\n",
        "\n",
        "# We will send it a bunch of records, and it will generate a question from each of them.\n",
        "\n",
        "# Note: we use a simple way of generating questions. We can use a more complex approach where we also track the source (filename) of the question. If we do it, we can later check if this file was retrieved and cited in the answer. But we won't do it today to make things simpler.\n",
        "\n"
      ],
      "metadata": {
        "id": "TDH35C5R8oP5"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's sample 10 records from our dataset using Python's built-in random.sample function:\n",
        "\n",
        "import random\n",
        "\n",
        "sample = random.sample(de_dtc_faq, 10)\n",
        "prompt_docs = [d['content'] for d in sample]\n",
        "prompt = json.dumps(prompt_docs)\n",
        "\n",
        "result = await question_generator.run(prompt)\n",
        "questions = result.output.questions\n",
        "\n",
        "print(questions)"
      ],
      "metadata": {
        "id": "0g3oqAf98oNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we simply iterate over each of the question, ask our agent and log the results:\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "for q in tqdm(questions):\n",
        "    print(q)\n",
        "\n",
        "    result = await agent.run(user_prompt=q)\n",
        "    print(result.output)\n",
        "\n",
        "    log_interaction_to_file(\n",
        "        agent,\n",
        "        result.new_messages(),\n",
        "        source='ai-generated'\n",
        "    )\n",
        "\n",
        "    print()\n",
        "\n",
        "# We can repeat it multiple times until we have enough data. Around 100 should be good for a start, but today we can just continue with the 10 log records we already generated.\n",
        "\n",
        "# Using AI for generating test data is quite powerful. It can help us get data faster and sometimes cover edge cases we won't think about.\n",
        "\n",
        "# There are limitations too:\n",
        "# - AI-generated questions might not reflect real user behavior\n",
        "# - It may miss important edge cases that only real users encounter\n",
        "# - They may not capture the full complexity of real user queries\n",
        "\n",
        "# The logs are ready, so we can run evaluation on them with our evaluation agent.\n"
      ],
      "metadata": {
        "id": "e3HGsEFca9di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ed0e21"
      },
      "source": [
        "# First, collect all the AI-generated logs for the v2 agent:\n",
        "\n",
        "\n",
        "eval_set = []\n",
        "\n",
        "for log_file in LOG_DIR.glob('*.json'):\n",
        "    if 'faq_agent_v2' not in log_file.name:\n",
        "        continue\n",
        "\n",
        "    log_record = load_log_file(log_file)\n",
        "    if log_record['source'] != 'ai-generated':\n",
        "        continue\n",
        "\n",
        "    eval_set.append(log_record)\n",
        "\n",
        "eval_results = []\n",
        "\n",
        "for log_record in tqdm(eval_set):\n",
        "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
        "    eval_results.append((log_record, eval_result))\n",
        "\n",
        "# This code:\n",
        "# - Loops through each AI-generated log\n",
        "# - Runs our evaluation agent on it\n",
        "# - Stores both the original log and evaluation result\n",
        "\n",
        "# There are ways to speed this up, but we won't cover them in detail here. For example, you can try this:\n",
        "# - Don't ask for justification - this makes evaluation faster but slightly lower quality\n",
        "# - Parallelize execution - you can ask ChatGPT how to do this with async/await\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91dcd236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b149c7c9-d06f-4b70-91c1-21c14189368c"
      },
      "source": [
        "# The results are collected, but we need to display them and also calculate some statistics. The best tool for doing this is Pandas. We already should have it because minsearch depends on it.\n",
        "\n",
        "# But we can make it an explicit dependency:\n",
        "\n",
        "!uv pip install pandas"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 250ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our data is not ready to be converted to a Pandas DataFrame. We first need to transform it a little. Let’s do it:\n",
        "\n",
        "rows = []\n",
        "\n",
        "for log_record, eval_result in eval_results:\n",
        "    messages = log_record['messages']\n",
        "\n",
        "    row = {\n",
        "        'file': log_record['log_file'].name,\n",
        "        'question': messages[0]['parts'][0]['content'],\n",
        "        'answer': messages[-1]['parts'][0]['content'],\n",
        "    }\n",
        "\n",
        "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
        "    row.update(checks)\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "# This code:\n",
        "# - Extracts key information from each log (file, question, answer)\n",
        "# - Converts the evaluation checks into a dictionary format\n",
        "\n",
        "# Now each row is a simple key-value dictionary, so we can create a DataFrame:\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_evals = pd.DataFrame(rows)\n",
        "\n",
        "# We can look at individual records and see which checks are False.\n",
        "\n",
        "# But it's also useful to look at the overall stats:\n",
        "\n",
        "df_evals.mean(numeric_only=True)\n",
        "\n",
        "# This calculates the average pass rate for each check:\n",
        "\n",
        "# instructions_follow    0.3\n",
        "# instructions_avoid     1.0\n",
        "# answer_relevant        1.0\n",
        "# answer_clear           1.0\n",
        "# answer_citations       0.3\n",
        "# completeness           0.7\n",
        "# tool_call_search       1.0\n",
        "\n",
        "# This tells us:\n",
        "# - Only 30% of responses follow instructions completely\n",
        "# - All responses avoid forbidden actions (good!)\n",
        "# - All responses are relevant and clear (great!)\n",
        "# - Only 30% include proper citations (needs improvement)\n",
        "# - 70% of responses are complete\n",
        "# - All responses use the search tool (as expected)\n",
        "\n",
        "# For us, the most important check is answer_relevant. This tells us whether the agent actually answers the user's question. If this score was low, it’d mean that our agent is not ready.\n",
        "\n",
        "# We now know how to evaluate our agent. What can we do with it now?\n",
        "\n",
        "# Many things:\n",
        "# - Decide if this quality is good enough for deployment\n",
        "# - Evaluate different chunking approaches and search\n",
        "# - See if changing a prompt leads to any improvements.\n",
        "\n",
        "# The algorithm is simple:\n",
        "# - Collect data for evaluation and keep this dataset fixed\n",
        "# - Run different versions of your agent for this dataset\n",
        "# - Compare key metrics to decide which version is better\n",
        "\n",
        "# Evaluation is a very powerful tool and we should use it when possible.\n",
        "\n"
      ],
      "metadata": {
        "id": "J-ffRWi7BaJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WynPX6kRg_vb"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating functions and tools\n",
        "\n",
        "# Also, we can (and should) evaluate our tools separately from evaluating the agent.\n",
        "\n",
        "# If it's code, we need to cover it with unit and integration tests.\n",
        "\n",
        "# We also have the search function, which we can evaluate using standard information retrieval metrics. For example:\n",
        "# - Precision and Recall: How many relevant results were retrieved vs. how many relevant results were missed\n",
        "# - Hit Rate: Percentage of queries that return at least one relevant result\n",
        "# - MRR (Mean Reciprocal Rank): Reflects the position of the first relevant result in the ranking\n",
        "\n",
        "# This is how we can implement hitrate and MRR calculation in Python:\n",
        "\n",
        "def evaluate_search_quality(search_function, test_queries):\n",
        "    results = []\n",
        "\n",
        "    for query, expected_docs in test_queries:\n",
        "        search_results = search_function(query, num_results=5)\n",
        "\n",
        "        # Calculate hit rate\n",
        "        relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
        "\n",
        "        # Calculate MRR\n",
        "        for i, doc in enumerate(search_results):\n",
        "            if doc['filename'] in expected_docs:\n",
        "                mrr = 1 / (i + 1)\n",
        "                break\n",
        "        else:\n",
        "            mrr = 0\n",
        "\n",
        "        results.append({\n",
        "            'query': query,\n",
        "            'hit': relevant_found,\n",
        "            'mrr': mrr\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# We won't do it today, but these ideas and the code will be useful when you implement a real agent project with search.\n",
        "\n",
        "# It's useful because it'll helps us make guided decisions about:\n",
        "# - When to use text vs. vector vs. hybrid search\n",
        "# - What are the best parameters for our search\n",
        "\n",
        "# You can ask ChatGPT to learn more about information retrieval evaluation metrics.\n",
        "\n",
        "# This was a very long lesson, but an important one. We finished it, and evaluated our agent. It’s good for deployment, so tomorrow we’ll create an UI for it and deploy it to the internet.\n"
      ],
      "metadata": {
        "id": "MPm8VB9LhCju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}