{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmantec/AI-Agents-Crash-Course/blob/feat%2FDay-1/Day-1/Day_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Kn76Mvk37YTJ"
      },
      "outputs": [],
      "source": [
        "## Intro\n",
        "\n",
        "# We'll create a conversational agent that can answer questions about any GitHub repository - think of it as your personal AI assistant for documentation and code.\n",
        "# If you know DeepWiki, it's something similar, but tailored to your GitHub repo.\n",
        "# For that, we need to:\n",
        "# - Download and process data from the repo\n",
        "# - Put it inside a search engine\n",
        "# - Make the search engine available to our agent\n",
        "\n",
        "# Today, we will do the first part: downloading the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jBcID2fJ7YPc"
      },
      "outputs": [],
      "source": [
        "## Ingest and Index Your Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuOG2O6T7YNr",
        "outputId": "77f6dc3d-599d-49db-b613-702607856ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uv in /usr/local/lib/python3.12/dist-packages (0.8.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install uv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeCFXtFQ7YKQ",
        "outputId": "2ef6e84a-b52a-43f3-fbe2-259db9e17e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 114ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install requests python-frontmatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNfOlUBf7YIi"
      },
      "outputs": [],
      "source": [
        "import frontmatter\n",
        "\n",
        "with open('sample_data/example.md', 'r', encoding='utf-8') as f:\n",
        "  post = frontmatter.load(f)\n",
        "\n",
        "# Access metadata\n",
        "print(post.metadata['title'])\n",
        "print(post.metadata['tags'])\n",
        "\n",
        "# Access content\n",
        "print(post.content)\n",
        "\n",
        "\n",
        "\n",
        "### Content of sample_data/example.md file\n",
        "\n",
        "# ---\n",
        "# title: \"Getting Started with AI\"\n",
        "# author: \"John Doe\"\n",
        "# date: \"2024-01-15\"\n",
        "# tags: [\"ai\", \"machine-learning\", \"tutorial\"]\n",
        "# difficulty: \"beginner\"\n",
        "# ---\n",
        "\n",
        "# # Getting Started with AI\n",
        "\n",
        "# This is the main content of the document written in **Markdown**.\n",
        "\n",
        "# You can include code blocks, links, and other formatting here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XVWgJgqs7YE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e42d21-bd55-467f-9897-93a21bf76ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Donâ€™t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}\n"
          ]
        }
      ],
      "source": [
        "## Working with Zip Archives\n",
        "\n",
        "# The second option is easier and more efficient for our use case.\n",
        "# We don't even need to save the zip archive - we can load it into our Python process memory and extract all the data we need from there.\n",
        "# So the plan:\n",
        "# - Use requests for downloading the zip archive from GitHub\n",
        "# - Open the archive using built-in zipfile and io modules\n",
        "# - Iterate over all .md and .mdx files in the repo\n",
        "# - Collect the results into a list\n",
        "\n",
        "# Let's implement it step by step.\n",
        "\n",
        "# First, we import the necessary libraries:\n",
        "\n",
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import frontmatter\n",
        "\n",
        "# Next, we download the repository as a zip file. Github privodes a convenient URL format for this:\n",
        "\n",
        "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
        "resp = requests.get(url)\n",
        "\n",
        "\n",
        "# Now we process the zip file in memory without saving it to disk:\n",
        "\n",
        "repository_data = []\n",
        "\n",
        "# Create a ZipFile object from the downloaded content\n",
        "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
        "\n",
        "for file_info in zf.infolist():\n",
        "  filename = file_info.filename.lower()\n",
        "\n",
        "  # Only process markdown files\n",
        "  if not filename.endswith('.md') or filename.endswith('.mdx'):\n",
        "    continue\n",
        "\n",
        "  # Read and parse each file\n",
        "  with zf.open(file_info) as f_in:\n",
        "    content = f_in.read()\n",
        "    post = frontmatter.loads(content)\n",
        "    data = post.to_dict()\n",
        "    data['filename'] = filename\n",
        "    repository_data.append(data)\n",
        "\n",
        "zf.close()\n",
        "\n",
        "\n",
        "# Let's look at what we got\n",
        "print(repository_data[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nJWLb34U7YDF"
      },
      "outputs": [],
      "source": [
        "# Complete implementation of above logic with reusable function\n",
        "\n",
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import frontmatter\n",
        "\n",
        "def read_repo_data(repo_owner, repo_name):\n",
        "  \"\"\"\n",
        "  Download and parse all markdown files from a github repository\n",
        "\n",
        "  Args:\n",
        "    repo_owner : Github username or organization\n",
        "    repo_name: Repository name\n",
        "\n",
        "  Returns:\n",
        "    List of dictionaries containing file content and metadata\n",
        "  \"\"\"\n",
        "  prefix = 'https://codeload.github.com'\n",
        "  url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
        "  resp = requests.get(url)\n",
        "\n",
        "  if resp.status_code != 200:\n",
        "    raise Exception(f\"Failed to download repository {repo_owner}/{repo_name}: {resp.status_code}\")\n",
        "\n",
        "  repository_data = []\n",
        "\n",
        "  # Create a ZipFile object from the downloaded content\n",
        "  zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
        "\n",
        "  for file_info in zf.infolist():\n",
        "    filename = file_info.filename\n",
        "    filename_lower = filename.lower()\n",
        "\n",
        "    if not (filename_lower.endswith('.md') or (filename_lower.endswith('.mdx'))):\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      with zf.open(file_info) as f_in:\n",
        "        content = f_in.read().decode('utf-8', errors='ignore')\n",
        "        post = frontmatter.loads(content)\n",
        "        data = post.to_dict()\n",
        "        data['filename'] = filename\n",
        "        repository_data.append(data)\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing {filename}: {e}\")\n",
        "      continue\n",
        "\n",
        "  zf.close()\n",
        "  return repository_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FhOhhxJu7X_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001865ce-cbb4-4a66-97b1-0982f5ce3fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAQ Documents: 1217\n",
            "Evidently Docs: 95\n"
          ]
        }
      ],
      "source": [
        "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
        "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
        "\n",
        "print(f\"FAQ Documents: {len(dtc_faq)}\")\n",
        "print(f\"Evidently Docs: {len(evidently_docs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEtOqk1V7X99"
      },
      "outputs": [],
      "source": [
        "# Data Processing Considerations\n",
        "\n",
        "# For FAQ, the data is ready to use. These are small records that we can index (put into a search engine) as is.\n",
        "# For Evidently docs, the documents are very large. We need extra processing called \"chunking\" - breaking large documents into smaller, manageable pieces. This is important because:\n",
        "\n",
        "# - Search relevance: Smaller chunks are more specific and relevant to user queries\n",
        "# - Performance: AI models work better with shorter text segments\n",
        "# - Memory limits: Large documents might exceed token limits of language models\n",
        "\n",
        "# We will cover chunking techniques in tomorrow's lesson.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyML3lG26TT667kqfmjqZ2R/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}