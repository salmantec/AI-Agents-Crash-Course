{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYDtkeGKPzS/E7qhLy5Cco",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb2f4c83043140a6b66106c11c2fd829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d80ab5e0c44445ea998f731a4e398e80",
              "IPY_MODEL_1f234b5af66e4ba8b3db4413899bd7a2",
              "IPY_MODEL_eaf8099af3ba48f7a3d47b39e22d637f"
            ],
            "layout": "IPY_MODEL_2217094b34474450bb059eb6298c345c"
          }
        },
        "d80ab5e0c44445ea998f731a4e398e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0caf4d0bfc4142739bb88fcea15a4762",
            "placeholder": "​",
            "style": "IPY_MODEL_592f4a160a0645eb9f0030eb73bce7bd",
            "value": "100%"
          }
        },
        "1f234b5af66e4ba8b3db4413899bd7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af42b728b8164948a19f1310610c916a",
            "max": 575,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a501fba2dc924e2ca60bc0cd410061c5",
            "value": 575
          }
        },
        "eaf8099af3ba48f7a3d47b39e22d637f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38d0ed32861047f0a005a4d7aef78c3b",
            "placeholder": "​",
            "style": "IPY_MODEL_d6359e84c0c445b1b412f38f3265156d",
            "value": " 575/575 [10:49&lt;00:00,  1.49it/s]"
          }
        },
        "2217094b34474450bb059eb6298c345c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0caf4d0bfc4142739bb88fcea15a4762": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "592f4a160a0645eb9f0030eb73bce7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af42b728b8164948a19f1310610c916a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a501fba2dc924e2ca60bc0cd410061c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38d0ed32861047f0a005a4d7aef78c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6359e84c0c445b1b412f38f3265156d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee593d9c841a4130b44919243089407b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3e05e370b0042d0a203a6147aee9690",
              "IPY_MODEL_c2ac5ef3cb6b4c608ca9f5cbb3c6ebb2",
              "IPY_MODEL_ece30ed777cd491abe7e23cfd35ceb0d"
            ],
            "layout": "IPY_MODEL_81b4e08c8d5d41b58a0016aa1893a18e"
          }
        },
        "c3e05e370b0042d0a203a6147aee9690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff219801d4e7407c88d85e54ac77c362",
            "placeholder": "​",
            "style": "IPY_MODEL_1c49ddad0a66414ca168c70120fd11e3",
            "value": "100%"
          }
        },
        "c2ac5ef3cb6b4c608ca9f5cbb3c6ebb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5212c5260e3d482692c28fa39f2c8b6d",
            "max": 449,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05ae879c918b49daa1430bd0711956bc",
            "value": 449
          }
        },
        "ece30ed777cd491abe7e23cfd35ceb0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_450ae1e34a1f43cf801f466552d76d83",
            "placeholder": "​",
            "style": "IPY_MODEL_05692894312c412abe204e91abc58058",
            "value": " 449/449 [03:02&lt;00:00,  1.85it/s]"
          }
        },
        "81b4e08c8d5d41b58a0016aa1893a18e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff219801d4e7407c88d85e54ac77c362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c49ddad0a66414ca168c70120fd11e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5212c5260e3d482692c28fa39f2c8b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05ae879c918b49daa1430bd0711956bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "450ae1e34a1f43cf801f466552d76d83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05692894312c412abe204e91abc58058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmantec/AI-Agents-Crash-Course/blob/feat%2FDay-3/Day-3/Day_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWr5oA9MeQ4E"
      },
      "outputs": [],
      "source": [
        "## Add search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In the first part of the course, we focus on data preparation. Before we can use data for AI agents, we need to prepare it properly.\n",
        "\n",
        "# We have already downloaded the data from a GitHub repository. Yesterday (Day 2), we processed it by chunking it where necessary.\n",
        "\n",
        "# Now it's time to use this data. We will index this data by putting it inside a search engine. This allows us to quickly find relevant information when users ask questions.\n",
        "\n",
        "# In particular, we will:\n",
        "# - Build a lexical search for exact matches and keywords\n",
        "# - Implement semantic search using embeddings\n",
        "# - Combine them with a hybrid search\n",
        "\n",
        "# At the end of this lesson, you'll have a working search system you can query about your project. This search engine can be used later by the AI agent to look up user questions in the database.\n"
      ],
      "metadata": {
        "id": "pfWfANKheedo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMiF4xAAhWWK",
        "outputId": "28f58907-def9-4623-ee26-50e1fc47b163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uv in /usr/local/lib/python3.12/dist-packages (0.8.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. Text search\n",
        "\n",
        "# The simplest type of search is a text search. Suppose we build a Q&A system for courses (using the FAQ dataset). We want to find the answer to this question:\n",
        "\n",
        "# \"What should be in a test dataset for AI evaluation?\"\n",
        "\n",
        "# Text search works by finding all documents that contain at least one word from the query. The more words from the query that appear in a document, the more relevant that document is.\n",
        "\n",
        "# This is how modern search systems like Apache Solr or Elasticsearch work. They use indexes to efficiently search through millions of documents without having to scan each one individually.\n",
        "\n",
        "# In this lesson, we'll start with a simple in-memory text search. The engine we will use is called minsearch.\n",
        "\n",
        "# Note: This search engine was implemented as part of a workshop I held some time ago. You can find details here if you want to know how it works\n",
        "\n",
        "!uv pip install minsearch requests python-frontmatter"
      ],
      "metadata": {
        "id": "aikkSYKueeZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12aa6a69-a2a6-446f-b2a7-e6c28878e71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 142ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  find read_repo_data in the first lesson and sliding_window in the second lesson\n",
        "\n",
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import frontmatter\n",
        "\n",
        "def read_repo_data(repo_owner, repo_name):\n",
        "  \"\"\"\n",
        "  Download and parse all markdown files from a github repository\n",
        "\n",
        "  Args:\n",
        "    repo_owner : Github username or organization\n",
        "    repo_name: Repository name\n",
        "\n",
        "  Returns:\n",
        "    List of dictionaries containing file content and metadata\n",
        "  \"\"\"\n",
        "  prefix = 'https://codeload.github.com'\n",
        "  url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
        "  resp = requests.get(url)\n",
        "\n",
        "  if resp.status_code != 200:\n",
        "    raise Exception(f\"Failed to download repository {repo_owner}/{repo_name}: {resp.status_code}\")\n",
        "\n",
        "  repository_data = []\n",
        "\n",
        "  # Create a ZipFile object from the downloaded content\n",
        "  zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
        "\n",
        "  for file_info in zf.infolist():\n",
        "    filename = file_info.filename\n",
        "    filename_lower = filename.lower()\n",
        "\n",
        "    if not (filename_lower.endswith('.md') or (filename_lower.endswith('.mdx'))):\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      with zf.open(file_info) as f_in:\n",
        "        content = f_in.read().decode('utf-8', errors='ignore')\n",
        "        post = frontmatter.loads(content)\n",
        "        data = post.to_dict()\n",
        "        data['filename'] = filename\n",
        "        repository_data.append(data)\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing {filename}: {e}\")\n",
        "      continue\n",
        "\n",
        "  zf.close()\n",
        "  return repository_data"
      ],
      "metadata": {
        "id": "mnhL9qdyeeYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_window(sequence, size, step):\n",
        "  if size <= 0 or step <= 0:\n",
        "    raise ValueError(\"Size and step must be positive\")\n",
        "\n",
        "  n = len(sequence)\n",
        "  result = []\n",
        "  for i in range(0, n, step):\n",
        "    chunk = sequence[i:i+size]\n",
        "    result.append({'start': i, 'chunk': chunk})\n",
        "    if i + size >= n:\n",
        "      break\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "MxuXVaSCeeTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
        "\n",
        "evidently_chunks = []\n",
        "\n",
        "for doc in evidently_docs:\n",
        "    doc_copy = doc.copy()\n",
        "    doc_content = doc_copy.pop('content')\n",
        "    chunks = sliding_window(doc_content, 2000, 1000)\n",
        "    for chunk in chunks:\n",
        "        chunk.update(doc_copy)\n",
        "    evidently_chunks.extend(chunks)\n",
        "\n",
        "# Let's now index this data with minsearch:\n",
        "\n",
        "from minsearch import Index\n",
        "\n",
        "index = Index(\n",
        "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
        "    keyword_fields=[]\n",
        ")\n",
        "\n",
        "index.fit(evidently_chunks)\n",
        "\n",
        "# Here we create an index that will search through four text fields: chunk content, title, description, and filename. The keyword_fields parameter is for exact matches (we don't need it for now).\n",
        "\n",
        "# We can now use it for search:\n",
        "\n",
        "query = 'What should be in a test dataset for AI evaluation?'\n",
        "results = index.search(query)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "F6T2GuEZlaUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76fbbbaf-6e8d-4b9f-f9a4-22e6f42c4338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'start': 0, 'chunk': 'Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.\\n\\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data.\\n\\n## Create a RAG test dataset\\n\\nYou can generate ground truth RAG dataset from your data source.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Upload your knowledge base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs. Choose how many inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the test cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>', 'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx'}, {'start': 3000, 'chunk': ' Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in t', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 2000, 'chunk': 'ho painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  - Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definiti', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 1000, 'chunk': ' run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import lte, gte, eq\\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />\\n\\n## 2. Prepare the dataset\\n\\nLet\\'s create a toy demo chatbot dataset with \"Questions\" and \"Answers\".\\n\\n```python\\ndata = [\\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n    [\"Tell me a joke.\", \"Why don\\'t programmers like nature? Too many bugs!\"],\\n    [\"When does water boil?\", \"Water\\'s boiling point is 100 degrees Celsius.\"],\\n    [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  -', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 8000, 'chunk': 'n LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Run and upload report\\n  report = Report([\\n      TextEvals()\\n  ])\\n  \\n  my_eval = report.run(llm_evals, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  # Uncomment to replace ws.add_run for a local preview \\n  # my_eval\\n  ```\\n\\n  You can implement any criteria this way, and plug in different LLM models.\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png)\\n\\n## What\\'s next?\\n\\nRead more on how you can configure [LLM judges for custom criteria or using other LLMs](/metrics/customize_llm_judge).\\n\\nWe also have lots of other examples\\\\! [Explore tutorials](/metrics/introduction).', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 5000, 'chunk': 'he notebook cell. You can also get a JSON or Python dictionary, or save as an external HTML file.\\n\\n```python\\n# my_eval.json()\\n# my_eval.dict()\\n# my_report.save_html(“file.html”)\\n```\\n\\nLocal Reports are great for quick experiments. To run comparisons, keep track of the results and collaborate with others, upload the results to Evidently Platform.\\n\\n**Upload the Report to Evidently Cloud** together with scored data:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\n**Explore.** Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, and navigate to Reports. You will see all score summaries and can browse the data. E.g. sort to find all answers labeled as \"Denials\".\\n\\n![](/images/examples/llm_quickstart_explore.png)\\n\\n## 5. Get a Dashboard \\n\\nAs you run more evals, it\\'s useful to track them over time. Go to \"Dashboard\" in the left menu, enter the \"Edit\" mode, and add a new \"Columns\" tab:\\n\\n![](/images/examples/llm_quickstart_create_tab_new.gif)\\n\\nYou\\'ll see a set of panels with descriptor values. Each will have a single data point for now. As you log more evaluation results, you can track trends and set up alerts.\\n\\nWant to see more complex workflows? You can add pass/fail conditions and custom evals.\\n\\n## 6. (Optional) Add tests\\n\\nYou can add conditions to your evaluations. For example, you may expect that:\\n\\n- **Sentiment** is non-negative (greater or equal to 0)\\n- **Text length** is at most 150 symbols (less or equal to 150).\\n- **Denials**: there are none.\\n- If any condition is false, consider the output to be a \"fail\".\\n\\nYou can implement this logic easily.\\n\\n<Accordion title=\"Add test conditions\" description=\"How to add test conditions\" icon=\"ballot-check\">\\n  ```python\\n  # Run the evaluation with tests \\n  eval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\",\\n                  tests=[gte(0, alias=\"Is_non_negative\")]),\\n        TextLength(\"answer\", ', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 0, 'chunk': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nEvidently helps you evaluate LLM outputs automatically. The lets you compare prompts, models, run regression or adversarial tests with clear, repeatable checks. That means faster iterations, more confident decisions, and fewer surprises in production.\\n\\nIn this Quickstart, you\\'ll try a simple eval in Python and view the results in Evidently Cloud. If you want to stay fully local, you can also do that - just skip a couple steps.\\n\\nThere are a few extras, like custom LLM judges or tests, if you want to go further.\\n\\nLet’s dive in.\\n\\n<Info>\\n  Need help at any point? Ask on [Discord](https://discord.com/invite/xZjKRaNp8b).\\n</Info>\\n\\n## 1. Set up your environment\\n\\nFor a fully local flow, skip steps 1.1 and 1.3.\\n\\n### 1.1. Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\n### 1.2. Installation and imports\\n\\nInstall the Evidently Python library:\\n\\n```python\\n!pip install evidently\\n```\\n\\nComponents to run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import lte, gte, eq\\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />\\n\\n## 2. Prepare the dataset\\n\\nLet\\'s create a toy demo chatbot dataset with \"Questions\" and \"Answers\".\\n\\n```python\\ndata = [\\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n    [\"Tell me a joke.\", \"Why don\\'t programmers like nature? Too many bugs!\"],\\n    [\"When does water boil?\", \"Water\\'s boiling point is 100 degrees Celsius.\"],\\n    [\"W', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 6000, 'chunk': 'et of panels with descriptor values. Each will have a single data point for now. As you log more evaluation results, you can track trends and set up alerts.\\n\\nWant to see more complex workflows? You can add pass/fail conditions and custom evals.\\n\\n## 6. (Optional) Add tests\\n\\nYou can add conditions to your evaluations. For example, you may expect that:\\n\\n- **Sentiment** is non-negative (greater or equal to 0)\\n- **Text length** is at most 150 symbols (less or equal to 150).\\n- **Denials**: there are none.\\n- If any condition is false, consider the output to be a \"fail\".\\n\\nYou can implement this logic easily.\\n\\n<Accordion title=\"Add test conditions\" description=\"How to add test conditions\" icon=\"ballot-check\">\\n  ```python\\n  # Run the evaluation with tests \\n  eval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\",\\n                  tests=[gte(0, alias=\"Is_non_negative\")]),\\n        TextLength(\"answer\", alias=\"Length\",\\n                   tests=[lte(150, alias=\"Has_expected_length\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\",\\n                       tests=[eq(\"OK\", column=\"Denials\",\\n                                 alias=\"Is_not_a_refusal\")]),\\n        TestSummary(success_all=True, alias=\"All_tests_passed\")])\\n  \\n  # Uncomment to preview the results locally\\n  # eval_dataset.as_dataframe()\\n  ```\\n\\n  ![](/images/examples/llm_quickstart_descriptor_tests-min.png)\\n\\n  You can limit the summary report to include only specific descriptor(s).\\n\\n  ```python\\n  report = Report([\\n      TextEvals(columns=[\"All_tests_passed\"])\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  #my_eval\\n  ```\\n\\n  To identify rows that failed any criteria, sort by \"All_test_passed\" column:\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_tests_report-min.png)\\n\\n## 7. (Optional) Add a custom LLM jugde\\n\\nYou can implement custom criteria using built-i', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 4000, 'chunk': 'on=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in the notebook cell. You can also get a JSON or Python dictionary, or save as an external HTML file.\\n\\n```python\\n# my_eval.json()\\n# my_eval.dict()\\n# my_report.save_html(“file.html”)\\n```\\n\\nLocal Reports are great for quick experiments. To run comparisons, keep track of the results and collaborate with others, upload the results to Evidently Platform.\\n\\n**Upload the Report to Evidently Cloud** together with scored data:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\n**Explore.** Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, and navigate to Reports. You will see all score summaries and can browse the data. E.g. sort to find all answers labeled as \"Denials\".\\n\\n![](/images/examples/llm_quickstart_explore.png)\\n\\n## 5. Get a Dashboard \\n\\nAs you run more evals, it\\'s useful to track them over time. Go to \"Dashboard\" in the left menu, enter the \"Edit\" mode, and add a new \"Columns\" tab:\\n\\n![](/images/examples/llm_quickstart_create_tab_new.gif)\\n\\nYou\\'ll see a s', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 7000, 'chunk': 'alias=\"Length\",\\n                   tests=[lte(150, alias=\"Has_expected_length\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\",\\n                       tests=[eq(\"OK\", column=\"Denials\",\\n                                 alias=\"Is_not_a_refusal\")]),\\n        TestSummary(success_all=True, alias=\"All_tests_passed\")])\\n  \\n  # Uncomment to preview the results locally\\n  # eval_dataset.as_dataframe()\\n  ```\\n\\n  ![](/images/examples/llm_quickstart_descriptor_tests-min.png)\\n\\n  You can limit the summary report to include only specific descriptor(s).\\n\\n  ```python\\n  report = Report([\\n      TextEvals(columns=[\"All_tests_passed\"])\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  #my_eval\\n  ```\\n\\n  To identify rows that failed any criteria, sort by \"All_test_passed\" column:\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_tests_report-min.png)\\n\\n## 7. (Optional) Add a custom LLM jugde\\n\\nYou can implement custom criteria using built-in LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Ru', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For DataTalksClub FAQ, it's similar, except we don't need to chunk the data. For the data engineering course, it'll look like this:\n",
        "\n",
        "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
        "\n",
        "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
        "\n",
        "faq_index = Index(\n",
        "    text_fields=[\"question\", \"content\"],\n",
        "    keyword_fields=[]\n",
        ")\n",
        "\n",
        "faq_index.fit(de_dtc_faq)\n",
        "\n",
        "query = 'Course: Can I still join the course after the start date?'\n",
        "results = index.search(query)\n",
        "print(results)\n",
        "\n",
        "# This is text search, also known as \"lexical search\". We look for exact matches between our query and the documents."
      ],
      "metadata": {
        "id": "eHvrMYRElP1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37be6f2-bdeb-4200-9f51-4f2228b60fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'start': 0, 'chunk': 'The result of each evaluation is a Report (summary of metrics with visuals) with an optional Test Suite (when it also includes pass/fail results on set conditions).\\n\\n**Browse the results**. To access the results of your evaluations, enter your Project and navigate to the \"Reports\" section in the left menu. Here, you can view all your evaluation artifacts and browse them by Tags, time, or metadata. You can also download them as HTML or JSON.\\n\\n![](/images/evals_browse_reports-min.png)\\n\\nTo see and compare the evaluation results, click on \"Explore\" next to the individual Report.\\n\\n**Explore view**. You\\'ll get the Report or Test Suite and, if available, the dataset linked to the evaluation.\\n\\n![](/images/evals_explore_view-min.png)\\n\\n- To view the Report only, click on the \"Dataset\" sign at the top to hide the dataset.\\n- To explore the Dataset only, choose \"Go to dataset\".\\n\\n**Compare**. To analyze multiple evaluation results side by side, simply select them from the Report list (e.g., reports generated using different LLMs) and click the **\"Compare\"** button. This allows you to quickly identify differences in performance, quality, or behavior across model versions or configurations.\\n\\n![](/images/platform_compare_select.png)\\n\\nYou will see the Compare view, where you can explore different metric scores (or pass/fail test results) side by side.\\n\\n![](/images/platform_compare_view.png)\\n\\nAlternatively, when you are viewing a specific Report, you can click on \"duplicate snapshot\" (this will keep the current Metric in view), and then select a different Report for comparison.\\n\\n**Track progress over time**. As you run multiple evaluations, you can build a Dashboard to track progress, see performance improvements, and monitor how tests perform over time. This will let you visualize results over time from multiple Reports within a Project. [Read more](/docs/platform/dashboard_overview).', 'title': 'Explore view', 'description': 'Reviewing the evaluation results on the Platform.', 'filename': 'docs-main/docs/platform/evals_explore.mdx'}, {'start': 5000, 'chunk': 'equired for text data drift detection.</li>            </ul>     | No automated mapping.                               |\\n\\n### ID and timestamp\\n\\nIf you have a timestamp or ID column, it\\'s useful to identify them.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n    )\\n```\\n\\n| **Column role** | **Description**                                                                                                             | **Automated mapping**    |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------ |\\n| `id_column`     | <ul>      <li>      Identifier column.</li>            <li>      Ignored in data drift calculations.</li>            </ul>  | Column named \"id\"        |\\n| `timestamp`     | <ul>      <li>      Timestamp column.</li>            <li>       Ignored in data drift calculations. </li>            </ul> | Column named \"timestamp\" |\\n\\n<Info>\\n  How is`timestamp` different from `datetime_columns`?\\n\\n  - **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like \"date of last contact.\"\\n  - **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.\\n</Info>\\n\\n### LLM evals\\n\\nWhen you generate [text descriptors](/docs/library/descriptors) and add them to the dataset, they are automatically mapped as `descriptors` in Data Definition. This means they will be included in the `TextEvals` [preset](/metrics/preset_text_evals) or treated as descriptors when you plot them on the dashboard.\\n\\nHowever, if you computed some scores or metadata externally and want to treat them as descriptors, you can map them explicitly:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descripto', 'title': 'Data definition', 'description': 'How to map the input data.', 'filename': 'docs-main/docs/library/data_definition.mdx'}, {'start': 1000, 'chunk': 'Target value, it will be evaluated together with other columns.\\n\\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\\n\\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\\n\\n![](/images/metrics/preset_data_drift-min.png)\\n\\nIf you choose to enable Tests, you will get an additional Test Suite view:\\n\\n![](/images/metrics/test_preset_data_drift-min.png)\\n\\n<Info>\\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\\n</Info>\\n\\n## Use case\\n\\nYou can evaluate data drift in different scenarios.\\n\\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\\n\\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\\n\\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\\n\\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\\n\\n<Info>\\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con', 'title': 'Data Drift', 'description': 'Overview of the Data Drift Preset.', 'filename': 'docs-main/metrics/preset_data_drift.mdx'}, {'start': 6000, 'chunk': 'fo>\\n  How is`timestamp` different from `datetime_columns`?\\n\\n  - **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like \"date of last contact.\"\\n  - **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.\\n</Info>\\n\\n### LLM evals\\n\\nWhen you generate [text descriptors](/docs/library/descriptors) and add them to the dataset, they are automatically mapped as `descriptors` in Data Definition. This means they will be included in the `TextEvals` [preset](/metrics/preset_text_evals) or treated as descriptors when you plot them on the dashboard.\\n\\nHowever, if you computed some scores or metadata externally and want to treat them as descriptors, you can map them explicitly:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descriptors=[\"upvotes\", \"model_type\"]\\n    )\\n```\\n\\n### Regression\\n\\nTo run regression quality checks, you must map the columns with:\\n\\n- Target: actual values.\\n- Prediction: predicted values.\\n\\nYou can have several regression results in the dataset, for example in case of multiple regression. (Pass the mappings in a list).\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\\n    )\\n```\\n\\nDefaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```\\n\\n### Classification\\n\\nTo run classification checks, you must map the columns with:\\n\\n- Target: true label.\\n- Prediction: predicted labels/probabilities.\\n\\nThere two different mapping options, for binary and multi-class classification. You can also have several classification results in the dataset. (Pass the mappings in a list).\\n\\n#### Multiclass\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import MulticlassClassification\\n\\ndata_def = DataDefinition(\\n    classification', 'title': 'Data definition', 'description': 'How to map the input data.', 'filename': 'docs-main/docs/library/data_definition.mdx'}, {'start': 0, 'chunk': '**Pre-requisites**:\\n* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\\n* You know how to create [Reports](/docs/library/report).\\n  \\n**Report.** To run a Preset on your data, comparing `current` data to `ref` data:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(),\\n])\\n\\nmy_eval = report.run(current, ref)\\n```\\n\\n**Test Suite.** To add Tests with explicit pass/fail for each column:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(),\\n],\\ninclude_tests=True)\\n\\nmy_eval = report.run(current, ref)\\n```\\n\\n## Overview\\n\\nThe`DataDriftPreset` lets you evaluate shift in data distribution between the two datasets to detect if there are significant changes.&#x20;\\n\\n![](/images/metrics/preset_data_drift_2-min.png)\\n\\n* **Column drift.** Checks for shifts in each column. The [drift detection method](/metrics/explainer_drift) is chosen automatically based on the column type and number of observations.\\n\\n* **Target / Prediction Drift**. If you dataset includes Prediction or Target value, it will be evaluated together with other columns.\\n\\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\\n\\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\\n\\n![](/images/metrics/preset_data_drift-min.png)\\n\\nIf you choose to enable Tests, you will get an additional Test Suite view:\\n\\n![](/images/metrics/test_preset_data_drift-min.png)\\n\\n<Info>\\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\\n</Info>\\n\\n## Use case\\n\\nYou can evaluate data drift in different scenarios.\\n\\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These ar', 'title': 'Data Drift', 'description': 'Overview of the Data Drift Preset.', 'filename': 'docs-main/metrics/preset_data_drift.mdx'}, {'start': 4000, 'chunk': 'lumns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\n\\n**Change drift parameters.** You can modify how drift detection works:\\n\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\n\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\n\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\n\\n<Info>\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\n</Info>\\n\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\n\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift(\"prediction\")` to your Report so that you see the drift in this value in a separate widget.\\n\\n* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive stats and run Tests like detecting missing values. Data drift check drops nulls (and compares the distributions of non-empty features), so you may want to run these Tests separately.\\n\\n* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.\\n\\n<Info>\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\n</Info>', 'title': 'Data Drift', 'description': 'Overview of the Data Drift Preset.', 'filename': 'docs-main/metrics/preset_data_drift.mdx'}, {'start': 30000, 'chunk': 'anels, you also specify the `field_path`. This helps point to a specific **result** inside the Metric. This can take the following values: `value` , `share`/`count` or `values` .\\n\\n| Field path         | Description                                          | Applicable Metrics                                                               | Applicable Panels |\\n| ------------------ | ---------------------------------------------------- | -------------------------------------------------------------------------------- | ----------------- |\\n| `value`            | Points to a single result from the Metric.           | Most Metrics                                                                     | Counter, Plot     |\\n| `share` or `count` | Points to either absolute count or percentage value. | Metrics that return both absolute and percentage values like `MissingValueCount` | Counter, Plot     |\\n| `shares` or `counts`        | Points to a histogram visualization within a Metric. | Metrics with histogram visualizations,  like `UniqueValueCount`.                 | Distribution      |\\n\\nThere are a few exceptions where a Metric can return a different result or a dictionary.\\n\\n<Note>\\n  **How to verify the result of a specific Metric?** Check in the [All Metrics table](/metrics/all_metrics). You can also generate the Report with a given Metric, export the Report as JSON and check the value name it returns.\\n</Note>\\n\\nWhen working in the Evidently Cloud, you can see available fields in the drop-down menu as you add a new Panel.', 'title': 'Dashboard panel types [Legacy]', 'description': 'Overview of the available monitoring Panels.', 'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'}, {'start': 5000, 'chunk': 'Reports. Each source Report contains a **single value** (e.g., a \"mean value\"). A Distribution Panel (`DashboardPanelDistribution`) shows how a distribution changes over time. Each source Report contains a **histogram** (e.g. frequency of different categories).\\n</Info>\\n\\n## What\\'s next?\\n\\nHow to add [monitoring Panels and Tabs](/docs/platform/dashboard_add_panels).\\n\\n---\\ntitle: \\'Add dashboard panels\\'\\ndescription: \\'How to design your Dashboard with custom Panels.\\'\\n---\\n\\nThis page shows how to add panels one by one. Check [pre-built Tabs](/docs/platform/dashboard_tabs) for a quick start, and explore [available Panel types](/docs/platform/dashboard_panel_types).\\n\\n## Adding Tabs\\n\\n<Check>\\n  Multiple Tabs are available in the Evidently Cloud and Enterprise.\\n</Check>\\n\\nBy default, new Panels appear on a single Dashboard. You can add multiple Tabs to organize them.\\n\\n**User interface.** Enter the \"Edit\" mode on the Dashboard (top right corner) and click the plus sign with \"add Tab\". To create a custom Tab, choose an “empty” tab and give it a name.\\n\\n**Python**. You can add an empty tab using `create_tab`:\\n\\n```python\\nproject.dashboard.create_tab(\"My tab\")\\nproject.save()\\n```\\n\\nYou can also use the `add_panel` method shown below and specify the destination Tab. If there is no Tab with a set name, you will create both a new Tab and Panel at once. If it already exists, a new Panel will appear below others in this Tab.\\n\\n## Adding Panels\\n\\nYou can add Panels in the user interface or using Python API.\\n\\n### User interface\\n\\n<Check>\\n  No-code Dashboards are available in the Evidently Cloud and Enterprise.\\n</Check>\\n\\nOnce you are inside the Project:\\n\\n* Enter the \"Edit\" mode by clicking on the top right corner of the Dashboard.\\n\\n* Click on the \"Add panel\" button.\\n\\n* Follow the flow to configure dashboard name, type, etc.\\n\\n* Preview and publish.\\n\\nTo delete/edit a Panel, enter Edit mode and hover over a specific Panel to choose an action.\\n\\n### Python API\\n\\n<Check>\\n  Dashboards as code are available ', 'title': 'Dashboard panel types [Legacy]', 'description': 'Overview of the available monitoring Panels.', 'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'}, {'start': 2000, 'chunk': 'e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\\n\\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\\n\\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\\n\\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\\n\\n<Info>\\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n</Info>\\n\\n## Data requirements\\n\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\n\\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\\n\\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report customization\\n\\nYou have multiple customization options.\\n\\n**Select co', 'title': 'Data Drift', 'description': 'Overview of the Data Drift Preset.', 'filename': 'docs-main/metrics/preset_data_drift.mdx'}, {'start': 3000, 'chunk': 'cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n</Info>\\n\\n## Data requirements\\n\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\n\\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\\n\\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report customization\\n\\nYou have multiple customization options.\\n\\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\n\\n**Change drift parameters.** You can modify how drift detection works:\\n\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\n\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\n\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\n\\n<Info>\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\n</Info>\\n\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\n\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in', 'title': 'Data Drift', 'description': 'Overview of the Data Drift Preset.', 'filename': 'docs-main/metrics/preset_data_drift.mdx'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Vector search\n",
        "\n",
        "# Text search has limitations. Consider these two queries:\n",
        "# - \"I just discovered the program, can I still enroll?\"\n",
        "# - \"I just found out about the course, can I still join?\"\n",
        "\n",
        "# These ask the same question but share no common words (among important ones). Text search would fail to find relevant matches.\n",
        "\n",
        "# This is where embeddings help. Embeddings are numerical representations of text that capture semantic meaning. Words and phrases with similar meanings have similar embeddings, even if they use different words.\n",
        "# Vector search uses these embeddings to identify semantically similar documents, rather than just exact word matches.\n",
        "\n",
        "# For vector search, we need to turn our documents into vectors (embeddings).\n",
        "\n",
        "# We will use the sentence-transformers library for this purpose.\n",
        "\n",
        "!uv pip install sentence-transformers"
      ],
      "metadata": {
        "id": "-m2Cp9twmkhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60021718-f9c1-411c-cfa5-96c655c3f2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 124ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
        "\n",
        "# The multi-qa-distilbert-cos-v1 model is trained explicitly for question-answering tasks. It creates embeddings optimized for finding answers to questions.\n",
        "\n",
        "# Other popular models include:\n",
        "# - all-MiniLM-L6-v2 - General-purpose, fast, and efficient\n",
        "# - all-mpnet-base-v2 - Higher quality, slower\n",
        "\n",
        "# Check Sentence Transformers documentation (https://www.sbert.net/docs/pretrained_models.html) for more options.\n",
        "\n",
        "# This is how we use it\n",
        "\n",
        "record = de_dtc_faq[2]\n",
        "text = record['question'] + ' ' + record['content']\n",
        "v_doc = embedding_model.encode(text)\n",
        "\n",
        "# We combine the question and answer text, then convert it to an embedding vector.\n",
        "\n",
        "# Let's do the same for the query:\n",
        "\n",
        "query = 'I just found out about the course. Can I enroll now?'\n",
        "v_query = embedding_model.encode(query)\n",
        "\n",
        "# This is how we compute similarity between the query and document vectors:\n",
        "similarity = v_query.dot(v_doc)\n",
        "\n",
        "# The dot product measures similarity between vectors\n",
        "\n",
        "# Values closer to 1 indicate higher similarity, closer to 0 means lower similarity. This works because the model creates normalized embeddings where cosine similarity equals the dot product.\n",
        "\n",
        "# So we can create embeddings for all documents, then compute similarity between the query and each document to find the most similar ones.\n",
        "\n",
        "# This is what VectorSearch from minsearch does. Let's use it.\n"
      ],
      "metadata": {
        "id": "DChC7-t2ms5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we turn our docs into embeddings. This process takes time, so we'll monitor progress with tqdm:\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "faq_embeddings = []\n",
        "\n",
        "for d in tqdm(de_dtc_faq):\n",
        "  text = d['question'] + ' ' + d['content']\n",
        "  v = embedding_model.encode(text)\n",
        "  faq_embeddings.append(v)\n",
        "\n",
        "faq_embeddings = np.array(faq_embeddings)\n",
        "\n",
        "# We combine question and answer text for each FAQ entry. We convert the list to a NumPy array for efficient similarity computations."
      ],
      "metadata": {
        "id": "zvm3rpV3mC7D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ee593d9c841a4130b44919243089407b",
            "c3e05e370b0042d0a203a6147aee9690",
            "c2ac5ef3cb6b4c608ca9f5cbb3c6ebb2",
            "ece30ed777cd491abe7e23cfd35ceb0d",
            "81b4e08c8d5d41b58a0016aa1893a18e",
            "ff219801d4e7407c88d85e54ac77c362",
            "1c49ddad0a66414ca168c70120fd11e3",
            "5212c5260e3d482692c28fa39f2c8b6d",
            "05ae879c918b49daa1430bd0711956bc",
            "450ae1e34a1f43cf801f466552d76d83",
            "05692894312c412abe204e91abc58058"
          ]
        },
        "outputId": "684b2850-6f6d-4209-fbe5-98f6f89a03d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/449 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee593d9c841a4130b44919243089407b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Now let's use VectorSearch:\n",
        "\n",
        "from minsearch import VectorSearch\n",
        "\n",
        "faq_vindex = VectorSearch()\n",
        "faq_vindex.fit(faq_embeddings, de_dtc_faq)\n",
        "\n",
        "# This creates a vector search index using our embeddings and original documents\n",
        "\n",
        "# Let's use it now\n",
        "\n",
        "query = 'Can I join the course now?'\n",
        "q = embedding_model.encode(query)\n",
        "results = faq_vindex.search(q)\n",
        "\n",
        "# We frist create an embedding for our query (q), then search for similar document embeddings.\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3BTYEsNIp1u",
        "outputId": "6c9b17dc-6e11-46ca-a69c-f3713ce17d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}, {'id': '900f60fd25', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'sort_order': 15, 'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'}, {'id': '721f9e0c29', 'question': 'How can we contribute to the course?', 'sort_order': 35, 'content': '- [Star the repository](https://github.com/DataTalksClub/data-engineering-zoomcamp).\\n- Share it with friends if you find it useful.\\n- Create a pull request (PR) if you can improve the text or structure of the repository.\\n- [Update this FAQ](https://github.com/DataTalksClub/faq/).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/035_721f9e0c29_how-can-we-contribute-to-the-course.md'}, {'id': '6314bc3029', 'images': [{'description': 'image #1', 'id': 'image_1', 'path': 'images/data-engineering-zoomcamp/image_1813f02b.png'}], 'question': 'How do I get my certificate?', 'sort_order': 46, 'content': 'There\\'ll be an announcement in Telegram and the course channel for:\\n\\n- Checking that your full name is displayed correctly on the Certificate (see Editing course profile on the Course Management webpage).\\n- Notifying when the grading is completed.\\n\\nYou will find it in your course profile (you need to be\\nlogged it). \\n\\nFor 2025 the link to the course profile is this:\\n\\n`https://courses.datatalks.club/de-zoomcamp-2025/enrollment`\\n\\nFor other editions, change \"2025\" to your edition.\\n\\nAfter the second announcement, follow instructions in [certificates.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md) on how to generate the Certificate document yourself.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/046_6314bc3029_how-do-i-get-my-certificate.md'}, {'id': '16005581f2', 'question': 'Edit Course Profile.', 'sort_order': 13, 'content': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname or your real name if you prefer. Your entry on the Leaderboard is the one highlighted in light green.\\n\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\n\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/013_16005581f2_edit-course-profile.md'}, {'id': 'b7542b8d36', 'question': 'Environment: Is the course [Windows/macOS/Linux/...] friendly?', 'sort_order': 36, 'content': 'Yes! Linux is ideal but technically it should not matter. Students in the 2024 cohort used all 3 OSes successfully.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/036_b7542b8d36_environment-is-the-course-windowsmacoslinux-friend.md'}, {'id': 'dc06a38bc6', 'question': 'How do I use Git / GitHub for this course?', 'sort_order': 42, 'content': 'After you create a GitHub account, clone the course repo to your local machine using the process outlined in this video:\\n\\n[Git for Everybody: How to Clone a Repository from GitHub](https://www.youtube.com/watch?v=CKcqniGu3tA).\\n\\nHaving this local repository on your computer will make it easy to access the instructors’ code and make pull requests if you want to add your own notes or make changes to the course content.\\n\\nYou will probably also create your own repositories to host your notes and versions of files. Here is a great tutorial that shows you how to do this:\\n\\n[How to Create a Git Repository](https://www.atlassian.com/git/tutorials/setting-up-a-repository).\\n\\nRemember to ignore large databases, .csv, and .gz files, and other files that should not be saved to a repository. Use `.gitignore` for this:\\n\\n[.gitignore file](https://www.atlassian.com/git/tutorials/saving-changes/gitignore).\\n\\n**Important:**\\n\\n**NEVER store passwords or keys in a git repo** (even if the repo is set to private). Put files containing sensitive information (.env, secret.json, etc.) in your `.gitignore`.\\n\\nThis is also a great resource: [Dangit, Git!?!](https://dangitgit.com/)', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/042_dc06a38bc6_how-do-i-use-git-github-for-this-course.md'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can easily do the same with the Evidently docs (but only use the chunk field for embeddings):\n",
        "\n",
        "evidently_embeddings = []\n",
        "\n",
        "for d in tqdm(evidently_chunks):\n",
        "  v = embedding_model.encode(d['chunk'])\n",
        "  evidently_embeddings.append(v)\n",
        "\n",
        "evidently_embeddings = np.array(evidently_embeddings)\n",
        "\n",
        "evidently_vindex = VectorSearch()\n",
        "evidently_vindex.fit(evidently_embeddings, evidently_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "eb2f4c83043140a6b66106c11c2fd829",
            "d80ab5e0c44445ea998f731a4e398e80",
            "1f234b5af66e4ba8b3db4413899bd7a2",
            "eaf8099af3ba48f7a3d47b39e22d637f",
            "2217094b34474450bb059eb6298c345c",
            "0caf4d0bfc4142739bb88fcea15a4762",
            "592f4a160a0645eb9f0030eb73bce7bd",
            "af42b728b8164948a19f1310610c916a",
            "a501fba2dc924e2ca60bc0cd410061c5",
            "38d0ed32861047f0a005a4d7aef78c3b",
            "d6359e84c0c445b1b412f38f3265156d"
          ]
        },
        "id": "o95UQdmsJeDX",
        "outputId": "11b5535e-8dec-41f7-8a8b-cce512a879bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/575 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb2f4c83043140a6b66106c11c2fd829"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<minsearch.vector.VectorSearch at 0x7c9866d77740>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Hybrid search\n",
        "\n",
        "# Text search is fast and efficient.\n",
        "# It works well for exact matches and specific terms, and requires no model inference. However, it misses semantically similar but differently worded queries and  struggles to handle synonyms effectively.\n",
        "\n",
        "# Vector search captures semantic meaning and handles paraphrased questions.\n",
        "# It works with synonyms and related concepts. But it may miss exact keyword matches\n",
        "\n",
        "# Combining both approaches gives us the best of both worlds. This is knows as \"Hybrid search\".\n",
        "\n",
        "# The code is quite simple\n",
        "\n",
        "query = 'Can I join the course now?'\n",
        "\n",
        "text_results = faq_index.search(query, num_results=5)\n",
        "\n",
        "q = embedding_model.encode(query)\n",
        "vector_results = faq_vindex.search(q, num_results=5)\n",
        "\n",
        "final_results = text_results + vector_results\n",
        "\n",
        "print(final_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ubwXw1FJ-KB",
        "outputId": "eb367ff3-f280-4c14-e32a-e0715d240625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '33fc260cd8', 'question': 'Course: What can I do before the course starts?', 'sort_order': 5, 'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}, {'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}, {'id': '900f60fd25', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'sort_order': 15, 'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting this together\n",
        "\n",
        "# Our search is implemented!\n",
        "\n",
        "# But before we can use it in our agent, we need to organize the code. Let's put all the code into different functions.\n",
        "\n",
        "def text_search(query):\n",
        "  return faq_index.search(query, num_results=5)\n",
        "\n",
        "def vector_search(query):\n",
        "  q = embedding_model.encode(query)\n",
        "  return faq_vindex.search(q, num_results=5)\n",
        "\n",
        "def hubrid_search(query):\n",
        "  text_results = text_search(query)\n",
        "  vector_results = vector_search(query)\n",
        "\n",
        "  # combine and deduplicate results\n",
        "  seen_ids = set()\n",
        "  combined_results = []\n",
        "\n",
        "  for result in text_results + vector_results:\n",
        "    if result['filename'] not in seen_ids:\n",
        "      seen_ids.add(result[['filename']])\n",
        "      combined_results.append(result)\n",
        "\n",
        "  return combined_results"
      ],
      "metadata": {
        "id": "-WZKR58MK8Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We have seen 3 approaches: text search, vector search, and hybrid search. You may wonder, how do I select the best one? We will discuss evaluation methods later in the course.\n",
        "\n",
        "# But like with chunking, you should always start with the simplest approach. For search, that's text search. It's faster, easier to debug, and works well for many use cases. Only add complexity when a simple text search isn't sufficient.\n",
        "\n",
        "# But let's first build our agent! Our data is ready. Tomorrow, we will build a conversational agent that can answer questions based on the data we collected.\n"
      ],
      "metadata": {
        "id": "mMJpwQeVMM5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}